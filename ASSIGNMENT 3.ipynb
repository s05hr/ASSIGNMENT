{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f472fb7",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4b020",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n",
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a27288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium     #import respective libraries for web scraping using selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver \n",
    "import warnings    \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86875d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver-\n",
    "driver=webdriver.Chrome(r\"D:\\chromedriver_win32.zip\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "#opening the amazon page on automated chrome browser\n",
    "driver.get(\"https://www.amazon.com/\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff8feeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter:pen\n"
     ]
    }
   ],
   "source": [
    "val=input(\"enter:\")  # input  from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d1c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send input data by user to search bar\n",
    "item=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "item.send_keys(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7389a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access to search button and click\n",
    "searchbar=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "searchbar.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a012ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to scrap the product url\n",
    "product_url=[]\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]')\n",
    "    for i in url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "    next_button=driver.find_elements(By.XPATH,'//a[@class=\"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"]')\n",
    "    next_button\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e4d985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "Name=[]\n",
    "Price=[]\n",
    "Return_=[]\n",
    "Delivery=[]\n",
    "Availability=[]\n",
    "\n",
    " # scrap data of given product using try and except block\n",
    "for url in product_url[:100]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element(By.XPATH,'//a[@id=\"bylineInfo\"]')  # scrap brand data from the given page\n",
    "        Brand.append(brand.text)\n",
    "        \n",
    "        name=driver.find_element(By.XPATH,'//span[@id=\"productTitle\"]')    # scrap name data from the given page\n",
    "        Name.append(name.text)\n",
    "        \n",
    "        price=driver.find_element(By.XPATH,'//span[@class=\"a-price a-text-price a-size-medium apexPriceToPay\"]') # scrap price data from the given page\n",
    "        Price.append(price.text)\n",
    "        \n",
    "        return_=driver.find_element(By.XPATH,'//a[@id=\"productSupportAndReturnPolicy-return-policy-anchor-text\"]')  # scrap return data from the given page\n",
    "        Return_.append(return_.text)\n",
    "        \n",
    "        delivery=driver.find_element(By.XPATH,'//div[@id=\"mir-layout-DELIVERY_BLOCK-slot-PRIMARY_DELIVERY_MESSAGE_LARGE\"]') # scrap delivery data from the given page\n",
    "        Delivery.append(delivery.text)\n",
    "        \n",
    "        availability=driver.find_element(By.XPATH,'//div[@id=\"availability\"]')# scrap availability data from the given page\n",
    "        Availability.append(availability.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "        Name.append('-')   \n",
    "        Price.append('-')\n",
    "        Return_.append('-')\n",
    "        Delivery.append('-')\n",
    "        Availability.append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c80f462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Return</th>\n",
       "      <th>Delivery</th>\n",
       "      <th>Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Visit the Amazon Basics Store</td>\n",
       "      <td>Amazon Basics Felt Tip Marker Pens - Assorted ...</td>\n",
       "      <td>$8.21</td>\n",
       "      <td>Eligible for Return, Refund or Replacement wit...</td>\n",
       "      <td>Delivery January 9 - February 2</td>\n",
       "      <td>In Stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Visit the novium Store</td>\n",
       "      <td>Novium Hoverpen - Luxury Pen for Men &amp; Women, ...</td>\n",
       "      <td>$72.00</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Visit the Asvine Store</td>\n",
       "      <td>Matte Black Forest Fountain Pen Extra Fine Nib...</td>\n",
       "      <td>$16.99</td>\n",
       "      <td>Eligible for Return, Refund or Replacement wit...</td>\n",
       "      <td>Delivery January 9 - February 2</td>\n",
       "      <td>In Stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Visit the Linbsunne Store</td>\n",
       "      <td>Linbsunne Black Ballpoint Pens Medium Point 1m...</td>\n",
       "      <td>$6.65</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Brand: Lin.Pen</td>\n",
       "      <td>Ballpoint Pens Medium Point 1mm Black Ink Work...</td>\n",
       "      <td>$30.88</td>\n",
       "      <td>Eligible for Return, Refund or Replacement wit...</td>\n",
       "      <td>Delivery January 9 - February 2</td>\n",
       "      <td>In Stock.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>$15.62</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Visit the Pentel Store</td>\n",
       "      <td>Pentel Libretto Roller Gel Pen, Rose Gold, Bla...</td>\n",
       "      <td>$31.99</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Brand  \\\n",
       "0   Visit the Amazon Basics Store   \n",
       "1          Visit the novium Store   \n",
       "2                               -   \n",
       "3          Visit the Asvine Store   \n",
       "4                               -   \n",
       "..                            ...   \n",
       "95      Visit the Linbsunne Store   \n",
       "96                              -   \n",
       "97                 Brand: Lin.Pen   \n",
       "98                              -   \n",
       "99         Visit the Pentel Store   \n",
       "\n",
       "                                                 Name   Price  \\\n",
       "0   Amazon Basics Felt Tip Marker Pens - Assorted ...   $8.21   \n",
       "1   Novium Hoverpen - Luxury Pen for Men & Women, ...  $72.00   \n",
       "2                                                   -       -   \n",
       "3   Matte Black Forest Fountain Pen Extra Fine Nib...  $16.99   \n",
       "4                                                   -       -   \n",
       "..                                                ...     ...   \n",
       "95  Linbsunne Black Ballpoint Pens Medium Point 1m...   $6.65   \n",
       "96                                                  -       -   \n",
       "97  Ballpoint Pens Medium Point 1mm Black Ink Work...  $30.88   \n",
       "98                                                  -  $15.62   \n",
       "99  Pentel Libretto Roller Gel Pen, Rose Gold, Bla...  $31.99   \n",
       "\n",
       "                                               Return  \\\n",
       "0   Eligible for Return, Refund or Replacement wit...   \n",
       "1                                                   -   \n",
       "2                                                   -   \n",
       "3   Eligible for Return, Refund or Replacement wit...   \n",
       "4                                                   -   \n",
       "..                                                ...   \n",
       "95                                                  -   \n",
       "96                                                  -   \n",
       "97  Eligible for Return, Refund or Replacement wit...   \n",
       "98                                                  -   \n",
       "99                                                  -   \n",
       "\n",
       "                           Delivery Availability  \n",
       "0   Delivery January 9 - February 2    In Stock.  \n",
       "1                                 -            -  \n",
       "2                                 -            -  \n",
       "3   Delivery January 9 - February 2    In Stock.  \n",
       "4                                 -            -  \n",
       "..                              ...          ...  \n",
       "95                                -            -  \n",
       "96                                -            -  \n",
       "97  Delivery January 9 - February 2    In Stock.  \n",
       "98                                -            -  \n",
       "99                                -            -  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the dataframe from above data-\n",
    "df=pd.DataFrame({'Brand':Brand[:100],'Name':Name[:100],'Price':Price[:100],'Return':Return_[:100],'Delivery':Delivery[:100],'Availability':Availability[:100]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d95a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b2f4f77",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3e3b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium     #import respective libraries for web scraping using selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver \n",
    "import warnings    \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e0dbd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver-\n",
    "driver=webdriver.Chrome(r\"D:\\chromedriver_win32.zip\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "#opening the google images page on automated chrome browser\n",
    "driver.get(\"https://images.google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae358e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item=driver.find_element(By.CLASS_NAME,\"gLFyf\") #access the search bar and send cars \n",
    "item.send_keys('Cars')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9703610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"wM6W7d\") #access the search button\n",
    "search.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aebd0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cars in range(10): \n",
    "    driver.execute_script(\"window.scrollBy(0,100)\")\n",
    "#scraping image from the given page\n",
    "images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15868e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 10 images\n",
      "Downloading 1 of 10 images\n",
      "Downloading 2 of 10 images\n",
      "Downloading 3 of 10 images\n",
      "Downloading 4 of 10 images\n",
      "Downloading 5 of 10 images\n",
      "Downloading 6 of 10 images\n",
      "Downloading 7 of 10 images\n",
      "Downloading 8 of 10 images\n",
      "Downloading 9 of 10 images\n",
      "Downloading 10 of 10 images\n"
     ]
    }
   ],
   "source": [
    "img_urls=[]   \n",
    "img_data=[]\n",
    "for image in images:   \n",
    "    source=image.get_attribute('src') #scrap src data from images\n",
    "    if source is not None:\n",
    "        if(source[0:4]=='http'):  \n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):  #scrap images \n",
    "    if i>10:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\".format(i,10)) # download images\n",
    "    response=requests.get(img_urls[i])\n",
    "    file=open(r\"C:\\Users\\91980\\OneDrive\\Desktop\\FlipRobo\"+str(i)+\".jpg\",\"wb\")\n",
    "    file.write(response.content)\n",
    "\n",
    "try: \n",
    "    images=driver.find_element(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    img_data.append(images.text)\n",
    "except NoSuchElementException:\n",
    "    img_data.append('not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0caf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8a21172",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af9f8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium     #import respective libraries for web scraping using selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver \n",
    "import warnings    \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2d6b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver-\n",
    "driver=webdriver.Chrome(r\"D:\\chromedriver_win32.zip\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "#opening the naukari page on automated chrome browser\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "31465bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "item=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "item.send_keys(\"Oneplus Nord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "93efb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "067bb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "for i in url[:10]:\n",
    "    product_url.append(i.get_attribute('href'))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "300e789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_name=[]\n",
    "Smartphone_name=[]\n",
    "Color=[]\n",
    "RAM=[]\n",
    "ROM=[]\n",
    "Primary_camera=[]\n",
    "Secondary_camera=[]\n",
    "Display_size=[]\n",
    "Battery_capacity=[]\n",
    "Price=[]\n",
    "\n",
    "for url in product_url:\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    try:\n",
    "        brand_name=driver.find_element(By.XPATH,'//div[@class=\"_1MR4o5\"]/div[4]/a')\n",
    "        Brand_name.append(brand_name.text)\n",
    "        \n",
    "        name=driver.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]')\n",
    "        Smartphone_name.append(name.text)\n",
    "        \n",
    "        color=driver.find_element(By.XPATH,'//div[@class=\"_1UhVsV\"]/div[1]/table/tbody/tr[4]/td[2]/ul/li')\n",
    "        Color.append(color.text)\n",
    "        \n",
    "        ram=driver.find_element(By.XPATH,'//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        RAM.append(ram.text)\n",
    "        \n",
    "        rom=driver.find_element(By.XPATH,'//div[@class=\"_1UhVsV\"]/div[4]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        ROM.append(rom.text)\n",
    "        \n",
    "        primary_camera=driver.find_element(By.XPATH,'//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "        Primary_camera.append(primary_camera.text)\n",
    "        \n",
    "        secondary_camera=driver.find_element(By.XPATH,'//div[@class=\"_1UhVsV\"]/div[5]/table/tbody/tr[5]/td[2]/ul/li')\n",
    "        Secondary_camera.append(secondary_camera.text)\n",
    "        \n",
    "        display_size=driver.find_element(By.XPATH,'//div[@class=\"_1UhVsV\"]/div[2]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Display_size.append( display_size.text)\n",
    "        \n",
    "        battery=driver.find_element(By.XPATH,'//div[@class=\"_1UhVsV\"]/div[10]/table/tbody/tr[1]/td[2]/ul/li')\n",
    "        Battery_capacity.append(battery.text)\n",
    "        \n",
    "        price=driver.find_element(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]')\n",
    "        Price.append(price.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Brand_name.append(\"-\")\n",
    "        Smartphone_name.append(\"-\")\n",
    "        Color.append(\"-\")\n",
    "        RAM.append(\"-\")\n",
    "        ROM.append(\"-\")\n",
    "        Primary_camera.append(\"-\")\n",
    "        Secondary_camera.append(\"-\")\n",
    "        Display_size.append(\"-\")\n",
    "        Battery_capacity.append(\"-\")\n",
    "        Price.append(\"-\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a1e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceec81e9",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e836590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium     #import respective libraries for web scraping using selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver \n",
    "import warnings    \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f362e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver-\n",
    "driver=webdriver.Chrome(r\"D:\\chromedriver_win32.zip\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "#opening the naukari page on automated chrome browser\n",
    "driver.get(\"https://www.google.com/maps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54106ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item=driver.find_element(By.XPATH,\"/html/body/div[3]/div[9]/div[3]/div[1]/div[1]/div/div[2]/form/input[1]\")\n",
    "item.send_keys(\"Banglore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5acd05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "item=driver.find_element(By.CLASS_NAME,\"DgCNMb\")\n",
    "item.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9b780e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL EXTRACTEd:  https://www.google.com/maps/place/Bengaluru,+Karnataka/@12.9538477,77.3507266,10z/data=!3m1!4b1!4m5!3m4!1s0x3bae1670c9b44e6d:0xf8dfc3e8517e4fe0!8m2!3d12.9715987!4d77.5945627\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url_string=[]\n",
    "try:\n",
    "    url_string=driver.current_url\n",
    "    print(\"URL EXTRACTEd: \",url_string)\n",
    "    lat_lng=re.findall(r'@(.*)data',url_string)\n",
    "except NoSuchElementException:\n",
    "    Brand_name.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ea1e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat_lng: ['12.9538477,77.3507266,10z/']\n",
      "12.9538477,77.3507266,10z/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['12.9538477', '77.3507266', '10z/']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in lat_lng:\n",
    "    print(\"lat_lng:\",lat_lng)\n",
    "    \n",
    "type(l)\n",
    "\n",
    "mystring=''\n",
    "for x in l:\n",
    "    mystring += ''+x\n",
    "print(mystring)\n",
    "mystring.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7200de64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12.9538477', '77.3507266', '10z/']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=mystring.split(\",\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8258bb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: 12.9538477\n"
     ]
    }
   ],
   "source": [
    "print(\"Latitude:\",a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f3ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitude: 77.3507266\n"
     ]
    }
   ],
   "source": [
    "print(\"Longitude:\",a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce503867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042d5a1e",
   "metadata": {},
   "source": [
    "7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "504a8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium     #import respective libraries for web scraping using selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver \n",
    "import warnings    \n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException \n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3dd4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connect to the driver-\n",
    "driver=webdriver.Chrome(r\"D:\\chromedriver_win32.zip\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "#opening the naukari page on automated chrome browser\n",
    "driver.get(\"https://www.digit.in/\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad7d7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[4]/ul/li[4]/a\")\n",
    "item.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73929e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item=driver.find_element(By.XPATH,\"/html/body/div[4]/div/div/div[3]/div[2]/ul/li[25]/a\")\n",
    "item.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a78a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Laptop_name=[]\n",
    "Windows=[]\n",
    "Display=[]\n",
    "Processor=[]\n",
    "Memory=[]\n",
    "Price=[]\n",
    "\n",
    "try:\n",
    "    l=driver.find_element(By.XPATH,'//div[@class=\"left_side\"]')\n",
    "    Laptop_name.append(l.text)\n",
    "    \n",
    "    w=driver.find_element(By.XPATH,\"//div[@class='Specs-Wrap']/ul/li[1]/div/div\")\n",
    "    Windows.append(w.text)\n",
    "    \n",
    "    d=driver.find_element(By.XPATH,\"//div[@class='Specs-Wrap']/ul/li[2]/div/div\")\n",
    "    Display.append(d.text)\n",
    "    \n",
    "    p=driver.find_element(By.XPATH,\"//div[@class='Specs-Wrap']/ul/li[3]/div/div\")\n",
    "    Processor.append(p.text)\n",
    "    \n",
    "    m=driver.find_element(By.XPATH,\"//div[@class='Specs-Wrap']/ul/li[4]/div/div\")\n",
    "    Memory.append(m.text)\n",
    "    \n",
    "    p=driver.find_element(By.XPATH,\"//div[@class='Spcs-details']/table/tbody/tr[9]/td[3]\")\n",
    "    Price.append(p.text)\n",
    "\n",
    "except NoSuchElementException:\n",
    "    Laptop_name.append(\"-\")\n",
    "    Windows.append(\"-\")\n",
    "    Display.append(\"-\")\n",
    "    Processor.append(\"-\")\n",
    "    Memory.append(\"-\")\n",
    "    Price.append(\"-\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'LaptopName':Laptop_name,'Windows':Windows,'Display':Display,'Processor':Processor,'Memory':Memory,'Price':Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed776fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b8b37f8",
   "metadata": {},
   "source": [
    "8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a72bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
