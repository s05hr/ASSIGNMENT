{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09baa63e",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1 --WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecac3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Main Page\n",
      "Welcome to Wikipedia\n",
      "From today's featured article\n",
      "Did you know ...\n",
      "In the news\n",
      "On this day\n",
      "Today's featured picture\n",
      "Other areas of Wikipedia\n",
      "Wikipedia's sister projects\n",
      "Wikipedia languages\n",
      "Navigation menu\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "Namespaces\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "Navigation\n",
      "\n",
      "\n",
      "Contribute\n",
      "\n",
      "\n",
      "Tools\n",
      "\n",
      "\n",
      "Print/export\n",
      "\n",
      "\n",
      "In other projects\n",
      "\n",
      "\n",
      "Languages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.Write a python program to display all the header tags from wikipedia.org.\n",
    "\n",
    "from bs4 import BeautifulSoup #importing the BeautifulSoup library\n",
    "import requests               #importing request library\n",
    "page=requests.get('https://en.wikipedia.org/wiki/Main_Page') #send get request to the webpage server to get the source code of the page\n",
    "print(page)\n",
    "soup=BeautifulSoup(page.content) #page content\n",
    "soup\n",
    "\n",
    "header_tags = [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"]       #creating list of all header tags\n",
    "for tags in soup.find_all(header_tags):            #loop for display all the header tags\n",
    "    print(tags.text)                               #print  text of header tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a73abc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie names:\n",
      " ['1.       The Shawshank Redemption (1994)', '2.       The Godfather (1972)', '3.       The Dark Knight (2008)', '4.       The Godfather Part II (1974)', '5.       12 Angry Men (1957)', \"6.       Schindler's List (1993)\", '7.       The Lord of the Rings: The Return of the King (2003)', '8.       Pulp Fiction (1994)', '9.       The Lord of the Rings: The Fellowship of the Ring (2001)', '10.       Il buono, il brutto, il cattivo (1966)', '11.       Forrest Gump (1994)', '12.       Fight Club (1999)', '13.       The Lord of the Rings: The Two Towers (2002)', '14.       Inception (2010)', '15.       The Empire Strikes Back (1980)', '16.       The Matrix (1999)', '17.       Goodfellas (1990)', \"18.       One Flew Over the Cuckoo's Nest (1975)\", '19.       Se7en (1995)', '20.       Shichinin no samurai (1954)', \"21.       It's a Wonderful Life (1946)\", '22.       The Silence of the Lambs (1991)', '23.       Cidade de Deus (2002)', '24.       Saving Private Ryan (1998)', '25.       La vita è bella (1997)', '26.       Interstellar (2014)', '27.       The Green Mile (1999)', '28.       Star Wars (1977)', '29.       Terminator 2: Judgment Day (1991)', '30.       Back to the Future (1985)', '31.       Sen to Chihiro no kamikakushi (2001)', '32.       Psycho (1960)', '33.       The Pianist (2002)', '34.       Gisaengchung (2019)', '35.       Léon (1994)', '36.       The Lion King (1994)', '37.       Gladiator (2000)', '38.       American History X (1998)', '39.       The Departed (2006)', '40.       The Usual Suspects (1995)', '41.       The Prestige (2006)', '42.       Whiplash (2014)', '43.       Casablanca (1942)', '44.       Seppuku (1962)', '45.       The Intouchables (2011)', '46.       Hotaru no haka (1988)', '47.       Modern Times (1936)', '48.       Once Upon a Time in the West (1968)', '49.       Rear Window (1954)', '50.       Nuovo Cinema Paradiso (1988)', '51.       Alien (1979)', '52.       City Lights (1931)', '53.       Apocalypse Now (1979)', '54.       Memento (2000)', '55.       Raiders of the Lost Ark (1981)', '56.       Django Unchained (2012)', '57.       WALL·E (2008)', '58.       The Lives of Others (2006)', '59.       Sunset Blvd. (1950)', '60.       Paths of Glory (1957)', '61.       The Great Dictator (1940)', '62.       The Shining (1980)', '63.       Avengers: Infinity War (2018)', '64.       Witness for the Prosecution (1957)', '65.       Aliens (1986)', '66.       Spider-Man: Into the Spider-Verse (2018)', '67.       American Beauty (1999)', '68.       Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', '69.       The Dark Knight Rises (2012)', '70.       Oldeuboi (2003)', '71.       Amadeus (1984)', '72.       Joker (2019)', '73.       Inglourious Basterds (2009)', '74.       Coco (2017)', '75.       Toy Story (1995)', '76.       Braveheart (1995)', '77.       Das Boot (1981)', '78.       Avengers: Endgame (2019)', '79.       Mononoke-hime (1997)', '80.       Once Upon a Time in America (1984)', '81.       Good Will Hunting (1997)', '82.       Top Gun: Maverick (2022)', '83.       Kimi no Na wa. (2016)', '84.       Requiem for a Dream (2000)', '85.       3 Idiots (2009)', \"86.       Singin' in the Rain (1952)\", '87.       Toy Story 3 (2010)', '88.       Tengoku to jigoku (1963)', '89.       Star Wars: Episode VI - Return of the Jedi (1983)', '90.       Capharnaüm (2018)', '91.       Eternal Sunshine of the Spotless Mind (2004)', '92.       2001: A Space Odyssey (1968)', '93.       Reservoir Dogs (1992)', '94.       Jagten (2012)', '95.       Idi i smotri (1985)', '96.       Citizen Kane (1941)', '97.       M - Eine Stadt sucht einen Mörder (1931)', '98.       Lawrence of Arabia (1962)', '99.       North by Northwest (1959)']\n",
      "year of release: ['(1994)', '(1972)', '(2008)', '(1974)', '(1957)', '(1993)', '(2003)', '(1994)', '(2001)', '(1966)', '(1994)', '(1999)', '(2002)', '(2010)', '(1980)', '(1999)', '(1990)', '(1975)', '(1995)', '(1954)', '(1946)', '(1991)', '(2002)', '(1998)', '(1997)', '(2014)', '(1999)', '(1977)', '(1991)', '(1985)', '(2001)', '(1960)', '(2002)', '(2019)', '(1994)', '(1994)', '(2000)', '(1998)', '(2006)', '(1995)', '(2006)', '(2014)', '(1942)', '(1962)', '(2011)', '(1988)', '(1936)', '(1968)', '(1954)', '(1988)', '(1979)', '(1931)', '(1979)', '(2000)', '(1981)', '(2012)', '(2008)', '(2006)', '(1950)', '(1957)', '(1940)', '(1980)', '(2018)', '(1957)', '(1986)', '(2018)', '(1999)', '(1964)', '(2012)', '(2003)', '(1984)', '(2019)', '(2009)', '(2017)', '(1995)', '(1995)', '(1981)', '(2019)', '(1997)', '(1984)', '(1997)', '(2022)', '(2016)', '(2000)', '(2009)', '(1952)', '(2010)', '(1963)', '(1983)', '(2018)', '(2004)', '(1968)', '(1992)', '(2012)', '(1985)', '(1941)', '(1931)', '(1962)', '(1959)']\n",
      "ratings: ['9.2', '9.2', '9.0', '9.0', '9.0', '8.9', '8.9', '8.8', '8.8', '8.8', '8.8', '8.7', '8.7', '8.7', '8.7', '8.7', '8.7', '8.6', '8.6', '8.6', '8.6', '8.6', '8.6', '8.6', '8.6', '8.6', '8.6', '8.6', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movienames</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Yearofrelease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.       The Shawshank Redemption (1994)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.       The Godfather (1972)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.       The Dark Knight (2008)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.       The Godfather Part II (1974)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1974)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.       12 Angry Men (1957)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.       Citizen Kane (1941)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1941)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.       M - Eine Stadt sucht einen Mörder (1...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1931)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.       Lawrence of Arabia (1962)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1962)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.       North by Northwest (1959)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(1959)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.       Vertigo (1958)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(1958)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movienames Ratings Yearofrelease\n",
       "0            1.       The Shawshank Redemption (1994)     9.2        (1994)\n",
       "1                       2.       The Godfather (1972)     9.2        (1972)\n",
       "2                     3.       The Dark Knight (2008)     9.0        (2008)\n",
       "3               4.       The Godfather Part II (1974)     9.0        (1974)\n",
       "4                        5.       12 Angry Men (1957)     9.0        (1957)\n",
       "..                                                ...     ...           ...\n",
       "95                      96.       Citizen Kane (1941)     8.3        (1941)\n",
       "96  97.       M - Eine Stadt sucht einen Mörder (1...     8.3        (1931)\n",
       "97                98.       Lawrence of Arabia (1962)     8.3        (1962)\n",
       "98                99.       North by Northwest (1959)     8.2        (1959)\n",
       "99                          100.       Vertigo (1958)     8.2        (1958)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)and make data frame.\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup   #importing BeautifulSoup library\n",
    "import requests                 #importing requests library\n",
    "import pandas as pd              #importing pandas library\n",
    "\n",
    "page=requests.get(\"https://www.imdb.com/chart/top/\")    #send get request to the webpage server to get the source code of the page\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content,\"html.parser\")       #page content\n",
    "soup.prettify()\n",
    "\n",
    "movies = []                   # make a variable \n",
    "\n",
    "for movie in soup.find_all('td',class_=\"titleColumn\"):   #for loop with tag line and class name of movies name\n",
    "    movie = movie.get_text().replace('\\n',\" \")      \n",
    "    movie = movie.strip(\" \")\n",
    "    movies.append(movie)   \n",
    "print(\"Movie names:\\n\",movies[:99])    #print 100 movies name\n",
    "\n",
    "\n",
    "year =[]  # make a variable for year of release\n",
    "for y in soup.find_all('span',class_=\"secondaryInfo\"):   #for loop with tag line and class name of year of release\n",
    "    year.append(y.get_text())\n",
    "print(\"year of release:\",year[:99])   # print 100 datas of year of release\n",
    "\n",
    "\n",
    "ratings=[]   # make a variable for ratings\n",
    "for r in soup.find_all('td',class_='ratingColumn imdbRating'):     #for loop with tag line and class name of ratings\n",
    "    ratings.append(r.get_text().strip())\n",
    "print(\"ratings:\",ratings[:99])      # print 100 datas of ratings\n",
    "\n",
    "data=pd.DataFrame({'movienames':movies,'Ratings': ratings,'Yearofrelease': year})    # make a dataframe of 100 datas\n",
    "data.loc[:99]    # print 100 datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09649174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies names:\n",
      " ['1.       Ramayana: The Legend of Prince Rama (1993)', '2.       Rocketry: The Nambi Effect (2022)', '3.       Golmaal (1979)', '4.       Nayakan (1987)', '5.       777 Charlie (2022)', '6.       Anbe Sivam (2003)', '7.       Jai Bhim (2021)', '8.       Pariyerum Perumal (2018)', '9.       3 Idiots (2009)', '10.       Apur Sansar (1959)', '11.       Manichitrathazhu (1993)', '12.       Kantara (2022)', '13.       #Home (2021)', '14.       Black Friday (2004)', '15.       Kumbalangi Nights (2019)', '16.       Soorarai Pottru (2020)', '17.       Taare Zameen Par (2007)', '18.       C/o Kancharapalem (2018)', '19.       Kireedam (1989)', '20.       Dangal (2016)', '21.       Kaithi (2019)', '22.       Jersey (2019)', '23.       96 (2018)', '24.       Asuran (2019)', '25.       Thevar Magan (1992)', '26.       Natsamrat (2016)', '27.       Drishyam 2 (2021)', '28.       Visaaranai (2015)', '29.       Thalapathi (1991)', '30.       Pather Panchali (1955)', '31.       Sita Ramam (2022)', '32.       Sarpatta Parambarai (2021)', '33.       Jaane Bhi Do Yaaro (1983)', '34.       Thani Oruvan (2015)', '35.       Sardar Udham (2021)', '36.       Aparajito (1956)', '37.       Drishyam (2013)', '38.       Khosla Ka Ghosla! (2006)', '39.       Vada Chennai (2018)', '40.       Ratsasan (2018)', '41.       Chupke Chupke (1975)', '42.       Anniyan (2005)', '43.       Peranbu (2018)', '44.       Mahanati (2018)', '45.       Satya (1998)', '46.       Gangs of Wasseypur (2012)', '47.       Bangalore Days (2014)', '48.       Premam (2015)', '49.       Agent Sai Srinivasa Athreya (2019)', '50.       Drishyam (2015)', '51.       Devasuram (1993)', '52.       Super Deluxe (2019)', '53.       Bhaag Milkha Bhaag (2013)', '54.       Tumbbad (2018)', '55.       Andhadhun (2018)', '56.       Vikram Vedha (2017)', '57.       Guide (1965)', '58.       Chithram (1988)', '59.       Vikram (2022)', '60.       Kannathil Muthamittal (2002)', '61.       Zindagi Na Milegi Dobara (2011)', '62.       Sairat (2016)', '63.       Shahid (2012)', '64.       Aruvi (2016)', '65.       Paan Singh Tomar (2012)', '66.       Iruvar (1997)', '67.       Chhichhore (2019)', '68.       Drishyam 2 (2022)', '69.       Swades: We, the People (2004)', '70.       Pyaasa (1957)', '71.       Chak De! India (2007)', '72.       Spadikam (1995)', '73.       Munna Bhai M.B.B.S. (2003)', '74.       Uri: The Surgical Strike (2019)', '75.       Mudhalvan (1999)', '76.       Black (2005)', '77.       Jo Jeeta Wohi Sikandar (1992)', '78.       Dhuruvangal Pathinaaru (2016)', '79.       Lagaan: Once Upon a Time in India (2001)', '80.       Hera Pheri (2000)', '81.       Papanasam (2015)', '82.       Pudhu Pettai (2006)', '83.       Queen (2013)', '84.       PK (2014)', '85.       Article 15 (2019)', '86.       Talvar (2015)', '87.       Sarfarosh (1999)', '88.       OMG: Oh My God! (2012)', '89.       Soodhu Kavvum (2013)', '90.       Mandela (2021)', '91.       Sholay (1975)', '92.       Udaan (2010)', '93.       Barfi! (2012)', '94.       Jigarthanda (2014)', '95.       The Legend of Bhagat Singh (2002)', '96.       Kaakkaa Muttai (2014)', '97.       Ustad Hotel (2012)', '98.       Theeran Adhigaaram Ondru (2017)', '99.       Angoor (1982)', '100.       Rang De Basanti (2006)']\n",
      "year of release: ['(1993)', '(2022)', '(1979)', '(1987)', '(2022)', '(2003)', '(2021)', '(2018)', '(2009)', '(1959)', '(1993)', '(2022)', '(2021)', '(2004)', '(2019)', '(2020)', '(2007)', '(2018)', '(1989)', '(2016)', '(2019)', '(2019)', '(2018)', '(2019)', '(1992)', '(2016)', '(2021)', '(2015)', '(1991)', '(1955)', '(2022)', '(2021)', '(1983)', '(2015)', '(2021)', '(1956)', '(2013)', '(2006)', '(2018)', '(2018)', '(1975)', '(2005)', '(2018)', '(2018)', '(1998)', '(2012)', '(2014)', '(2015)', '(2019)', '(2015)', '(1993)', '(2019)', '(2013)', '(2018)', '(2018)', '(2017)', '(1965)', '(1988)', '(2022)', '(2002)', '(2011)', '(2016)', '(2012)', '(2016)', '(2012)', '(1997)', '(2019)', '(2022)', '(2004)', '(1957)', '(2007)', '(1995)', '(2003)', '(2019)', '(1999)', '(2005)', '(1992)', '(2016)', '(2001)', '(2000)', '(2015)', '(2006)', '(2013)', '(2014)', '(2019)', '(2015)', '(1999)', '(2012)', '(2013)', '(2021)', '(1975)', '(2010)', '(2012)', '(2014)', '(2002)', '(2014)', '(2012)', '(2017)', '(1982)']\n",
      "ratings: ['8.5', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0', '8.0']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movienames</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Yearofrelease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.       Ramayana: The Legend of Prince Rama (...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.       Rocketry: The Nambi Effect (2022)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.       Golmaal (1979)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.       Nayakan (1987)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(1987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.       777 Charlie (2022)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.       Kaakkaa Muttai (2014)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.       Ustad Hotel (2012)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.       Theeran Adhigaaram Ondru (2017)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.       Angoor (1982)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(1982)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.       Rang De Basanti (2006)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>(2006)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movienames Ratings Yearofrelease\n",
       "0   1.       Ramayana: The Legend of Prince Rama (...     8.5        (1993)\n",
       "1          2.       Rocketry: The Nambi Effect (2022)     8.4        (2022)\n",
       "2                             3.       Golmaal (1979)     8.4        (1979)\n",
       "3                             4.       Nayakan (1987)     8.4        (1987)\n",
       "4                         5.       777 Charlie (2022)     8.4        (2022)\n",
       "..                                                ...     ...           ...\n",
       "95                    96.       Kaakkaa Muttai (2014)     8.0        (2014)\n",
       "96                       97.       Ustad Hotel (2012)     8.0        (2012)\n",
       "97          98.       Theeran Adhigaaram Ondru (2017)     8.0        (2017)\n",
       "98                            99.       Angoor (1982)     8.0        (1982)\n",
       "99                  100.       Rang De Basanti (2006)     8.0        (2006)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup     # import respective libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://www.imdb.com/india/top-rated-indian-movies/\")  # send request to get the source code\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content,\"html.parser\")   # page content\n",
    "soup.prettify()\n",
    "\n",
    "\n",
    "movies = []   # make  a variable \n",
    "\n",
    "for movie in soup.find_all('td',class_=\"titleColumn\"):   # for loop with tag line and column name of movies name\n",
    "    movie = movie.get_text().replace('\\n',\" \")\n",
    "    movie = movie.strip(\" \")\n",
    "    movies.append(movie)\n",
    "    \n",
    "print(\"Movies names:\\n\",movies[:100])    # print 100 datas\n",
    "\n",
    "\n",
    "year =[]     # make a variable\n",
    "for y in soup.find_all('span',class_=\"secondaryInfo\"):   #for loop with tag line and column name of year of release\n",
    "    year.append(y.get_text())\n",
    "    \n",
    "print(\"year of release:\",year[:99])   #print 100 datas of year of release\n",
    "\n",
    "\n",
    "ratings=[] # make a variable\n",
    "for r in soup.find_all('td',class_='ratingColumn imdbRating'):    #for loop with tag line and column name of ratings\n",
    "    ratings.append(r.get_text().strip())\n",
    "print(\"ratings:\",ratings[:99])    #print 100 datas of ratings\n",
    "\n",
    "data=pd.DataFrame({'movienames':movies,'Ratings':ratings,'Yearofrelease':year})    # make a dataframe\n",
    "data.loc[:99]  # print dataframe in the form of table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe55d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President_name:\n",
      " ['Shri Ram Nath Kovind (birth - 1945)', 'Shri Pranab Mukherjee (1935-2020)', 'Smt Pratibha Devisingh Patil (birth - 1934)', 'DR. A.P.J. Abdul Kalam (1931-2015)', 'Shri K. R. Narayanan (1920 - 2005)', 'Dr Shankar Dayal Sharma (1918-1999)', 'Shri R Venkataraman (1910-2009)', 'Giani Zail Singh (1916-1994)', 'Shri Neelam Sanjiva Reddy (1913-1996)', 'Dr. Fakhruddin Ali Ahmed (1905-1977)', 'Shri Varahagiri Venkata Giri (1894-1980)', 'Dr. Zakir Husain (1897-1969)', 'Dr. Sarvepalli Radhakrishnan (1888-1975)', 'Dr. Rajendra Prasad (1884-1963)']\n",
      "Terms of office:\n",
      " \n",
      " ['25 July, 2017 to 25 July, 2022', '25 July, 2012 to 25 July, 2017', '25 July, 2007 to 25 July, 2012', '25 July, 2002 to 25 July, 2007', '25 July, 1997 to 25 July, 2002', '25 July, 1992 to 25 July, 1997', '25 July, 1987 to 25 July, 1992', '25 July, 1982 to 25 July, 1987', '25 July, 1977 to 25 July, 1982', '24 August, 1974 to 11 February, 1977', '3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974', '13 May, 1967 to 3 May, 1969', '13 May, 1962 to 13 May, 1967', '26 January, 1950 to 13 May, 1962'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President_name</th>\n",
       "      <th>Terms_of_office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind (birth - 1945)</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh (1916-1994)</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963)</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 President_name  \\\n",
       "0           Shri Ram Nath Kovind (birth - 1945)   \n",
       "1             Shri Pranab Mukherjee (1935-2020)   \n",
       "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
       "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
       "4            Shri K. R. Narayanan (1920 - 2005)   \n",
       "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
       "6               Shri R Venkataraman (1910-2009)   \n",
       "7                  Giani Zail Singh (1916-1994)   \n",
       "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
       "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
       "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
       "11                 Dr. Zakir Husain (1897-1969)   \n",
       "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
       "13              Dr. Rajendra Prasad (1884-1963)   \n",
       "\n",
       "                                      Terms_of_office  \n",
       "0                      25 July, 2017 to 25 July, 2022  \n",
       "1                      25 July, 2012 to 25 July, 2017  \n",
       "2                      25 July, 2007 to 25 July, 2012  \n",
       "3                      25 July, 2002 to 25 July, 2007  \n",
       "4                      25 July, 1997 to 25 July, 2002  \n",
       "5                      25 July, 1992 to 25 July, 1997  \n",
       "6                      25 July, 1987 to 25 July, 1992  \n",
       "7                      25 July, 1982 to 25 July, 1987  \n",
       "8                      25 July, 1977 to 25 July, 1982  \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm\n",
    "\n",
    "from bs4 import BeautifulSoup     # import respective libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\") # send requests to get source code\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)   # page content\n",
    "soup\n",
    "\n",
    "president_name=[]     # make a variable\n",
    "terms=[]      # make a variable\n",
    "\n",
    "\n",
    "for name in soup.find_all('div',class_=\"presidentListing\"):   # scrapped president name\n",
    "    president_name.append(name.h3.get_text().strip())\n",
    "    \n",
    "print(\"President_name:\\n\",president_name)   # print president name\n",
    "\n",
    "\n",
    "for term in soup.find_all('div',class_=\"presidentListing\"):     # scrapped terms of office\n",
    "    terms.append(term.p.get_text().strip(\"Term of Office:\"))\n",
    "    \n",
    "print(\"Terms of office:\\n\",\"\\n\",terms,\"\\n\")     # print terms of office\n",
    "\n",
    "data=pd.DataFrame({'President_name':president_name,'Terms_of_office':terms})   # make a dataframe\n",
    "data.loc[:]  # print data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb9eaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[30]</td>\n",
       "      <td>[3,400]</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[32]</td>\n",
       "      <td>[3,572]</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[35]</td>\n",
       "      <td>[3,866]</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[22]</td>\n",
       "      <td>[2,354]</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[24]</td>\n",
       "      <td>[2,392]</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[30]</td>\n",
       "      <td>[2,753]</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[30]</td>\n",
       "      <td>[2,677]</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[19]</td>\n",
       "      <td>[1,380]</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[41]</td>\n",
       "      <td>[2,902]</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[23]</td>\n",
       "      <td>[1,214]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[27]</td>\n",
       "      <td>[1,254]</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Matches   Points Ratings\n",
       "0     [30]  [3,400]     113\n",
       "1     [32]  [3,572]     112\n",
       "2     [35]  [3,866]     110\n",
       "3     [22]  [2,354]     107\n",
       "4     [24]  [2,392]     100\n",
       "5     [30]  [2,753]      92\n",
       "6     [30]  [2,677]      89\n",
       "7     [19]  [1,380]      73\n",
       "8     [41]  [2,902]      71\n",
       "9     [23]  [1,214]      53\n",
       "10    [27]  [1,254]      46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    " #a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    " #b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    " #c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "    \n",
    "#5a.\n",
    "\n",
    "from bs4 import BeautifulSoup    # print respective libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")  # send  request to get source code\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)   # page content\n",
    "soup\n",
    "\n",
    "matches=[]    # make an empty variables\n",
    "points=[]   # make an empty variables\n",
    "rating=[]   # make an empty variables\n",
    "\n",
    "\n",
    "scrapped_items=soup.find_all('td',class_=\"table-body__cell u-center-text\")    # scrapped items from specific class\n",
    "scrapped_items     \n",
    "\n",
    "for i in range(0,len(scrapped_items)-1,2):   # for loop to the scrapped matches and points data\n",
    "    matches.append(scrapped_items[i])\n",
    "    points.append(scrapped_items[i+1])\n",
    "\n",
    "for r in soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\"):  #for loop to scrap the rating data\n",
    "     rating.append(r.get_text().strip())\n",
    "        \n",
    "\n",
    "data=pd.DataFrame({'Matches':matches,'Points':points,'Ratings':rating})    # make a dataframe\n",
    "data.loc[:10]    # print 10 datas in table format\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbf4e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      " Teams: \n",
      " ['PAK', 'SA', 'SA', 'AUS', 'AUS', 'ENG', 'IND', 'IND', 'NZ', 'PAK']\n",
      "\n",
      " Ratings: \n",
      " ['779', '766', '759', '747', '719', '710', '707', '704', '701', '690']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teams</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUS</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUS</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENG</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IND</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IND</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NZ</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAK</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WI</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Teams Ratings\n",
       "0    PAK     779\n",
       "1     SA     766\n",
       "2     SA     759\n",
       "3    AUS     747\n",
       "4    AUS     719\n",
       "5    ENG     710\n",
       "6    IND     707\n",
       "7    IND     704\n",
       "8     NZ     701\n",
       "9    PAK     690\n",
       "10    WI     679"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5b\n",
    "\n",
    "from bs4 import BeautifulSoup    # import respective libaries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")  # send request to get the source code\n",
    "print(page)\n",
    "\n",
    "soup=BeautifulSoup(page.content)   # page content\n",
    "soup\n",
    "\n",
    "team=[]   # make an empty variable\n",
    "for t in soup.find_all('span',class_=\"table-body__logo-text\"):   #for loop to scrapped data from team\n",
    "    team.append(t.get_text())\n",
    "print(\"\\n Teams: \\n\",team[:10])   #print \n",
    "\n",
    "rating=[]     # make an empty variable\n",
    "for r in soup.find_all('td',class_=\"table-body__cell rating\"):   #for loop to scrapped data from ratings\n",
    "    rating.append(r.get_text())\n",
    "print(\"\\n Ratings: \\n\",rating[:10])\n",
    "\n",
    "data=pd.DataFrame({'Teams':team,'Ratings':rating})    #make a dataframe \n",
    "data.loc[:10]   # print data in table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9415b9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      " Teams: \n",
      " ['AUS', 'AUS', 'PAK', 'NZ', 'AUS', 'BAN', 'AFG', 'BAN', 'AFG', 'ENG']\n",
      "\n",
      " Ratings: \n",
      " ['727', '665', '661', '656', '655', '655', '650', '640', '635', '632']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teams</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUS</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUS</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NZ</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUS</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AFG</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BAN</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AFG</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ENG</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AFG</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Teams Ratings\n",
       "0    AUS     727\n",
       "1    AUS     665\n",
       "2    PAK     661\n",
       "3     NZ     656\n",
       "4    AUS     655\n",
       "5    BAN     655\n",
       "6    AFG     650\n",
       "7    BAN     640\n",
       "8    AFG     635\n",
       "9    ENG     632\n",
       "10   AFG     631"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5c\n",
    "\n",
    "from bs4 import BeautifulSoup    # importing respective libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")   # send requests to  get source code\n",
    "print(page)\n",
    "\n",
    "soup=BeautifulSoup(page.content) # page content\n",
    "soup\n",
    "\n",
    "team=[]   #  make an empty variable\n",
    "for t in soup.find_all('span',class_=\"table-body__logo-text\"):  #for loop for scrapped data from team\n",
    "    team.append(t.get_text())\n",
    "print(\"\\n Teams: \\n\",team[:10])   # print data in an empty variables\n",
    "\n",
    "rating=[]     #make an empty variable\n",
    "for r in soup.find_all('td',class_=\"table-body__cell rating\"):   # for loop for scrapped data from rating\n",
    "    rating.append(r.get_text())\n",
    "print(\"\\n Ratings: \\n\",rating[:10])   #print data in an empty variables\n",
    " \n",
    "data=pd.DataFrame()    # make a dataframe\n",
    "data['Teams']=team\n",
    "data['Ratings']=rating\n",
    "\n",
    "data.loc[:10]   # print 10 data in table format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c42f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[26]</td>\n",
       "      <td>[3,098]</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25]</td>\n",
       "      <td>[2,904]</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[27]</td>\n",
       "      <td>[2,820]</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[24]</td>\n",
       "      <td>[2,425]</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[24]</td>\n",
       "      <td>[2,334]</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[12]</td>\n",
       "      <td>[932]</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[8]</td>\n",
       "      <td>[572]</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[24]</td>\n",
       "      <td>[1,519]</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[8]</td>\n",
       "      <td>[353]</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[14]</td>\n",
       "      <td>[548]</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[9]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Matches   Points Ratings\n",
       "0     [26]  [3,098]     119\n",
       "1     [25]  [2,904]     116\n",
       "2     [27]  [2,820]     104\n",
       "3     [24]  [2,425]     101\n",
       "4     [24]  [2,334]      97\n",
       "5     [12]    [932]      78\n",
       "6      [8]    [572]      72\n",
       "7     [24]  [1,519]      63\n",
       "8      [8]    [353]      44\n",
       "9     [14]    [548]      39\n",
       "10     [9]      [0]       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "#a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "#6a.\n",
    "\n",
    "from bs4 import BeautifulSoup    # import respective libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")   #send request to get source code\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)   #page content\n",
    "soup\n",
    "\n",
    "matches=[]    #make an empty variable matches\n",
    "points=[]     #make an empty variable points\n",
    "rating=[]     #make an empty variable rating\n",
    "\n",
    "\n",
    "scrapped_items=soup.find_all('td',class_=\"table-body__cell u-center-text\")   #scrapped data from a specific class \n",
    "scrapped_items\n",
    "\n",
    "for i in range(0,len(scrapped_items)-1,2):  #for loop to scrap data and put it in an empty variables \n",
    "    matches.append(scrapped_items[i])\n",
    "    points.append(scrapped_items[i+1])\n",
    "\n",
    "for r in soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\"):  #for loop to scrap rating data and put in an empty variable rating\n",
    "     rating.append(r.get_text().strip())\n",
    "\n",
    "\n",
    "data=pd.DataFrame()   # make dataframe\n",
    "data['Matches']=matches\n",
    "data['Points']=points\n",
    "data['Ratings']=rating\n",
    "\n",
    "data.loc[:10]  # print 10 data using loc function in the table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbe2a921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      " Teams: \n",
      " ['AUS', 'SA', 'ENG', 'IND', 'IND', 'AUS', 'AUS', 'NZ', 'SL', 'AUS']\n",
      "\n",
      " Ratings: \n",
      " ['749', '732', '725', '716', '714', '710', '701', '661', '655', '642']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teams</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AUS</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ENG</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Teams Ratings\n",
       "0    AUS     749\n",
       "1     SA     732\n",
       "2    ENG     725\n",
       "3    IND     716\n",
       "4    IND     714\n",
       "5    AUS     710\n",
       "6    AUS     701\n",
       "7     NZ     661\n",
       "8     SL     655\n",
       "9    AUS     642\n",
       "10   ENG     609"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6b\n",
    "\n",
    "from bs4 import BeautifulSoup   # import respective libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "page=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\") # send requests to get the source code\n",
    "print(page)\n",
    "\n",
    "soup=BeautifulSoup(page.content)   # page content\n",
    "soup\n",
    "\n",
    "team=[]   # an empty variable\n",
    "for t in soup.find_all('span',class_=\"table-body__logo-text\"):   #for loop to scrap data and put in an empty variable team\n",
    "    team.append(t.get_text())\n",
    "print(\"\\n Teams: \\n\",team[:10])  # print 10 data in team variable\n",
    "\n",
    "rating=[]     # an empty variable\n",
    "for r in soup.find_all('td',class_=\"table-body__cell rating\"):    # for loop to scrap data and put in an empty variable rating\n",
    "    rating.append(r.get_text())\n",
    "print(\"\\n Ratings: \\n\",rating[:10])  # print 10 data in rating variable\n",
    "\n",
    "data=pd.DataFrame()   # make dataframe\n",
    "data['Teams']=team\n",
    "data['Ratings']=rating\n",
    "\n",
    "data.loc[:10]   # print 10 data using loc function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad2d984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      " Teams: \n",
      " ['AUS', 'ENG', 'NZ', 'SA', 'IND', 'AUS', 'AUS', 'IND', 'ENG', 'PAK']\n",
      "\n",
      " Ratings: \n",
      " ['374', '357', '356', '349', '322', '270', '246', '214', '207', '205']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teams</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NZ</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ENG</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PAK</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ENG</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Teams Ratings\n",
       "0    AUS     374\n",
       "1    ENG     357\n",
       "2     NZ     356\n",
       "3     SA     349\n",
       "4    IND     322\n",
       "5    AUS     270\n",
       "6    AUS     246\n",
       "7    IND     214\n",
       "8    ENG     207\n",
       "9    PAK     205\n",
       "10   ENG     201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6c\n",
    "\n",
    "from bs4 import BeautifulSoup    # # import respective libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "page=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\") # send requests to get the source code\n",
    "print(page)\n",
    "\n",
    "soup=BeautifulSoup(page.content) # page content\n",
    "soup\n",
    "\n",
    "team=[]  # an empty variable\n",
    "for t in soup.find_all('span',class_=\"table-body__logo-text\"): # for loop to scrap data and put in an empty variable team\n",
    "    team.append(t.get_text())\n",
    "print(\"\\n Teams: \\n\",team[:10])  # print 10 data in team variable\n",
    "\n",
    "rating=[]   # an empty variable\n",
    "for r in soup.find_all('td',class_=\"table-body__cell rating\"):   # for loop to scrap data and put in an empty variable team\n",
    "    rating.append(r.get_text())\n",
    "print(\"\\n Ratings: \\n\",rating[:10])   # print 10 data in rating variable\n",
    "\n",
    "data=pd.DataFrame()  # make dataframe\n",
    "data['Teams']=team\n",
    "data['Ratings']=rating\n",
    "\n",
    "data.loc[:10]   # print 10 data using loc function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e205d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      " Headlines: \n",
      " ['Manufacturing orders from China down 40% in demand collapse', \"Asset manager names 9 'cheap' stocks to buy as recession fears grow\", 'Celsius clients with collateral stuck on failed crypto platform turn to bankruptcy court for relief', 'Startup backed by a Tesla investor says it can deliver a $300,000 flying car by 2025', \"Amazon's cloud unit faces cost-sensitive customers as economic fears mount\", \"Op-ed: Authoritarian rulers suffered new setbacks in 2022. Here's what the democratic world needs to do to seize the momentum\", 'Big Tech is set for a comeback, asset manager says, naming 2 stocks to get ahead of it', 'These 10 cars have the greatest potential lifespan — and 6 are Toyotas', 'Chinese F1 Grand Prix canceled for 2023', 'Beijing, Shenzhen loosen more Covid curbs as China easing gathers pace', 'The 10 countries with the least paid vacation—the U.S. is No. 2', \"35% of millionaires say it's 'going to take a miracle' to be ready for retirement, report finds\", 'Indonesia evacuates villagers as volcano erupts on Java island', 'Netherlands eliminates U.S. in round 16 of the World Cup', \"The Fed's path to a 'Goldilocks' economy just got a little more complicated\", 'Pentagon debuts its new stealth bomber, the B-21 Raider', \"The U.S. wants the EU to be strict with China. But Europe can't afford it\", 'U.S. expects reduced tempo in Ukraine fighting to continue for months', \"CNBC Pro Talks: Evercore's Mark Mahaney finds bargains in big tech and answers your questions\", 'Tech layoffs send visa holders on frantic search for employment to avoid deportation']\n",
      "\n",
      " Time: \n",
      " ['4 hours ago', '4 hours ago', '4 hours ago', '4 hours ago', '4 hours ago', '4 hours ago', '3 hours ago', '3 hours ago']\n",
      "\n",
      " Newslink: \n",
      " ['#MainContent', '//www.cnbc.com/world/', '/', '/markets/', '/pre-markets/', '/us-markets/', '/markets-europe/', '/china-markets/', '/markets-asia-pacific/', '/world-markets/', '/currencies/', '/cryptocurrency/', '/futures-and-commodities/', '/bonds/', '/funds-and-etfs/', '/business/', '/economy/', '/finance/', '/health-and-science/', '/media/', '/real-estate/', '/energy/', '/climate/', '/transportation/', '/industrials/', '/retail/', '/wealth/', '/life/', '/small-business/', '/investing/', '/personal-finance/', '/fintech/', '/financial-advisors/', '/options-action/', '/etf-street/', 'https://buffett.cnbc.com', '/earnings/', '/trader-talk/', '/technology/', '/cybersecurity/', '/enterprise/', '/internet/', '/media/', '/mobile/', '/social-media/', '/cnbc-disruptors/', '/tech-guide/', '/politics/', '/white-house/', '/policy/', '/defense/', '/congress/', '/equity-opportunity/', '/europe-politics/', '/china-politics/', '/asia-politics/', '/world-politics/', '/tv/', '/live-audio/', '/latest-video/', '/top-video/', '/video-ceo-interviews/', '/europe-television/', '/asia-business-day/', '/podcast/', '/digital-original/', '/watchlist/', '/investingclub/', '/investingclub/charitable-trust/', '/investingclub/analysis/', '/investingclub/trade-alerts/', '/investingclub/video/', '/investingclub/education/', '/pro/', '/pro/news/', '/pro/', '#', '#', '/make-it/', '/?region=usa', '/world/', '/watchlist/', '#', '#', '/', '/markets/', '/business/', '/investing/', '/technology/', '/politics/', '/tv/', '/watchlist/', '/investingclub/', '/pro/', '#', '#', '#', '#', '#', 'https://www.cnbc.com/2022/12/02/european-markets-open-to-close-as-investors-react-to-us-jobs-data.html', 'https://www.cnbc.com/2022/12/01/stock-market-futures-open-to-close-news.html', 'https://www.cnbc.com/quotes/US10Y', 'https://www.cnbc.com/2022/12/01/tesla-ceo-elon-musk-kicks-off-semi-truck-deliveries.html', 'https://www.cnbc.com/2022/11/30/bitcoin-family-moving-more-than-1-million-into-dexs-after-ftx-collapse.html', 'https://www.cnbc.com/2022/11/30/economist-chinas-zero-covid-has-shaken-confidence-in-supply-chains.html', 'https://www.cnbc.com/2022/11/30/former-ftx-ceo-sam-bankman-fried-says-i-didnt-ever-try-to-commit-fraud.html', 'https://www.cnbc.com/2022/12/04/opec-meeting-oil-producer-group-in-focus-ahead-of-russia-sanctions.html', 'https://www.cnbc.com/2022/12/04/opec-meeting-oil-producer-group-in-focus-ahead-of-russia-sanctions.html', 'https://www.cnbc.com/2022/12/02/russia-oil-price-cap-g-7-outline-how-it-is-going-to-work.html', 'https://www.cnbc.com/2022/12/03/ukraine-urges-tougher-western-squeeze-on-russian-oil-prices.html', 'https://www.cnbc.com/2022/12/04/us-intel-chief-on-russia-using-up-ammunition-in-ukraine.html', 'https://www.cnbc.com/2022/12/04/us-intel-chief-on-russia-using-up-ammunition-in-ukraine.html', 'https://www.cnbc.com/2022/12/04/irans-attorney-general-signals-that-morality-police-could-be-abolished.html', 'https://www.cnbc.com/2022/12/04/irans-attorney-general-signals-that-morality-police-could-be-abolished.html', 'https://www.cnbc.com/2022/12/04/the-10-countries-with-the-least-paid-vacationthe-us-is-no-2.html', 'https://www.cnbc.com/2022/12/04/michigan-couple-teaches-people-how-to-start-lucrative-side-hustles.html', 'https://www.cnbc.com/2022/12/04/these-10-cars-have-the-greatest-potential-lifespan.html', 'https://www.cnbc.com/2022/12/04/alef-aeronautics-startup-backed-by-tesla-investor-flying-car-by-2025.html', 'https://www.cnbc.com/2022/12/04/manufacturing-orders-from-china-down-40percent-in-demand-collapse.html', '/pro/', 'https://www.cnbc.com/2022/12/04/death-of-the-internal-combustion-engine-cowens-best-stocks-to-play-the-trend.html', '/pro/', 'https://www.cnbc.com/2022/12/04/these-stocks-are-cheap-heading-into-2023-and-analysts-love-them.html', 'https://www.cnbc.com/2022/12/04/disney-ceo-top-contenders-succeed-bob-iger.html', 'https://www.cnbc.com/2022/12/04/study-exercise-may-increase-the-effectiveness-of-covid-19-vaccines.html', 'https://www.cnbc.com/2022/12/04/opec-meeting-oil-producer-group-in-focus-ahead-of-russia-sanctions.html', 'https://www.cnbc.com/2022/12/03/parking-lots-becoming-as-important-as-cars-in-climate-change-efforts.html', 'https://www.cnbc.com/2022/12/03/aws-faces-cost-sensitive-customers-at-reinvent-as-economic-fears-mount.html', 'https://www.cnbc.com/2022/12/03/dont-overlook-this-health-and-safety-warning-on-your-holiday-lights.html', 'https://www.cnbc.com/2022/12/03/how-electric-air-taxis-could-shake-up-the-airline-industry.html', 'https://www.cnbc.com/2022/12/03/i-raised-2-successful-ceos-and-a-doctor-heres-the-no-1-skill-parents-are-failing-to-teach-kids-today.html', 'https://www.cnbc.com/2022/12/03/delta-pilots-would-get-more-than-30percent-in-pay-raises-under-new-contract-deal.html', 'https://www.cnbc.com/2022/12/03/men-participate-less-in-401k-plans-than-women-unless-auto-enrolled.html', 'https://www.cnbc.com/2022/12/03/best-states-raise-a-family-rocket-mortgage.html', 'https://www.cnbc.com/2022/12/03/bestselling-author-susan-cain-how-to-make-tough-conversations-easier.html', '/pro/', 'https://www.cnbc.com/2022/12/03/the-difference-between-this-comeback-and-the-markets-last-failed-bear-market-bounce.html', '/pro/', 'https://www.cnbc.com/2022/12/03/goldman-says-buy-these-five-stocks-in-a-weakening-macro-environment.html', 'https://www.cnbc.com/2022/12/03/celsius-users-with-crypto-collateral-stuck-turn-to-bankruptcy-process.html', 'https://www.cnbc.com/2022/12/02/cramers-lightning-round-let-extreme-networks-cool-off-before-buying.html', 'https://www.cnbc.com/2022/12/02/jim-cramer-says-these-3-apparel-stocks-benefit-from-return-to-office.html', 'https://www.cnbc.com/2022/12/02/cramers-week-ahead-markets-need-a-strong-job-market-tame-inflation.html', '/pro/', 'https://www.cnbc.com/2022/12/02/pro-picks-watch-all-of-fridays-big-stock-calls-on-cnbc.html', '/pro/', 'https://www.cnbc.com/2022/12/02/reits-offer-enormous-opportunity-says-gilman-hills-jenny-harrington.html', 'https://www.cnbc.com/2022/12/02/biden-administration-will-end-monkeypox-public-health-emergency.html', '/investingclub/', 'https://www.cnbc.com/2022/12/02/expect-more-choppiness-ahead-after-a-week-of-mixed-market-signals.html', 'https://www.cnbc.com/2022/12/02/gm-lg-investing-275-million-to-expand-tennessee-ev-battery-plant.html', '/us-market-movers/', 'https://www.cnbc.com/2022/12/04/manufacturing-orders-from-china-down-40percent-in-demand-collapse.html', 'https://www.cnbc.com/lori-ann-larocco/', 'https://www.cnbc.com/2022/12/04/manufacturing-orders-from-china-down-40percent-in-demand-collapse.html', 'https://www.cnbc.com/lori-ann-larocco/', '/pro/', 'https://www.cnbc.com/2022/11/29/asset-manager-names-9-cheap-stocks-to-buy-as-recession-fears-grow.html', 'https://www.cnbc.com/weizhen-tan/', 'https://www.cnbc.com/2022/11/29/asset-manager-names-9-cheap-stocks-to-buy-as-recession-fears-grow.html', 'https://www.cnbc.com/weizhen-tan/', 'https://www.cnbc.com/2022/12/03/celsius-users-with-crypto-collateral-stuck-turn-to-bankruptcy-process.html', 'https://www.cnbc.com/paige-tortorelli/', 'https://www.cnbc.com/kate-rooney/', 'https://www.cnbc.com/2022/12/03/celsius-users-with-crypto-collateral-stuck-turn-to-bankruptcy-process.html', 'https://www.cnbc.com/paige-tortorelli/', 'https://www.cnbc.com/kate-rooney/', 'https://www.cnbc.com/2022/12/04/alef-aeronautics-startup-backed-by-tesla-investor-flying-car-by-2025.html', 'https://www.cnbc.com/tom-huddleston-jr/', 'https://www.cnbc.com/2022/12/04/alef-aeronautics-startup-backed-by-tesla-investor-flying-car-by-2025.html', 'https://www.cnbc.com/tom-huddleston-jr/', 'https://www.cnbc.com/2022/12/03/aws-faces-cost-sensitive-customers-at-reinvent-as-economic-fears-mount.html', 'https://www.cnbc.com/jordan-novet/', 'https://www.cnbc.com/2022/12/03/aws-faces-cost-sensitive-customers-at-reinvent-as-economic-fears-mount.html', 'https://www.cnbc.com/jordan-novet/', 'https://www.cnbc.com/2022/12/04/op-ed-authoritarian-rulers-suffered-new-setbacks-in-2022.html', 'https://www.cnbc.com/fred-kempe/', 'https://www.cnbc.com/2022/12/04/op-ed-authoritarian-rulers-suffered-new-setbacks-in-2022.html', 'https://www.cnbc.com/fred-kempe/', '/pro/', 'https://www.cnbc.com/2022/11/29/market-pro-says-big-tech-is-set-for-a-comeback-and-names-2-stocks-he-owns-.html', 'https://www.cnbc.com/zavier-ong/', 'https://www.cnbc.com/2022/11/29/market-pro-says-big-tech-is-set-for-a-comeback-and-names-2-stocks-he-owns-.html', 'https://www.cnbc.com/zavier-ong/', 'https://www.cnbc.com/2022/12/04/these-10-cars-have-the-greatest-potential-lifespan.html', 'https://www.cnbc.com/2022/12/04/these-10-cars-have-the-greatest-potential-lifespan.html', 'https://www.cnbc.com/2022/12/02/chinese-f1-grand-prix-canceled-for-2023.html', 'https://www.cnbc.com/2022/12/02/chinese-f1-grand-prix-canceled-for-2023.html', 'https://www.cnbc.com/2022/12/03/beijing-shenzhen-loosen-more-covid-curbs-as-china-easing-gathers-pace.html', 'https://www.cnbc.com/2022/12/03/beijing-shenzhen-loosen-more-covid-curbs-as-china-easing-gathers-pace.html', 'https://www.cnbc.com/2022/12/04/the-10-countries-with-the-least-paid-vacationthe-us-is-no-2.html', 'https://www.cnbc.com/ashton-jackson/', 'https://www.cnbc.com/2022/12/04/the-10-countries-with-the-least-paid-vacationthe-us-is-no-2.html', 'https://www.cnbc.com/ashton-jackson/', 'https://www.cnbc.com/2022/12/02/35percent-of-millionaires-say-they-wont-have-enough-to-retire-report-finds.html', 'https://www.cnbc.com/jessica-dickler/', 'https://www.cnbc.com/2022/12/02/35percent-of-millionaires-say-they-wont-have-enough-to-retire-report-finds.html', 'https://www.cnbc.com/jessica-dickler/', 'https://www.cnbc.com/2022/12/04/japan-monitoring-possible-tsunami-risk-from-indonesia-volcano-report.html', 'https://www.cnbc.com/2022/12/04/japan-monitoring-possible-tsunami-risk-from-indonesia-volcano-report.html', 'https://www.cnbc.com/2022/12/03/netherlands-eliminates-us-in-round-16-of-the-world-cup.html', 'https://www.cnbc.com/2022/12/03/netherlands-eliminates-us-in-round-16-of-the-world-cup.html', 'https://www.cnbc.com/2022/12/02/the-feds-path-to-a-goldilocks-economy-just-got-more-complicated.html', 'https://www.cnbc.com/jeff-cox/', 'https://www.cnbc.com/2022/12/02/the-feds-path-to-a-goldilocks-economy-just-got-more-complicated.html', 'https://www.cnbc.com/jeff-cox/', 'https://www.cnbc.com/2022/12/03/pentagon-debuts-its-new-stealth-bomber-the-b-21-raider.html', 'https://www.cnbc.com/2022/12/03/pentagon-debuts-its-new-stealth-bomber-the-b-21-raider.html', 'https://www.cnbc.com/2022/12/02/the-us-wants-the-eu-to-be-strict-with-china-but-europe-cant-afford-it.html', 'https://www.cnbc.com/silvia-amaro/', 'https://www.cnbc.com/2022/12/02/the-us-wants-the-eu-to-be-strict-with-china-but-europe-cant-afford-it.html', 'https://www.cnbc.com/silvia-amaro/', 'https://www.cnbc.com/2022/12/04/us-expects-reduced-tempo-in-ukraine-fighting-to-continue-for-months.html', 'https://www.cnbc.com/2022/12/04/us-expects-reduced-tempo-in-ukraine-fighting-to-continue-for-months.html', '/pro/', 'https://www.cnbc.com/video/2022/12/02/cnbc-pro-talks-mark-mahaney-finds-bargains-in-big-tech.html', 'https://www.cnbc.com/yun-li/', 'https://www.cnbc.com/yun-li/', 'https://www.cnbc.com/2022/12/02/tech-layoffs-leave-visa-holders-scrambling-for-jobs-to-remain-in-us.html', 'https://www.cnbc.com/annie-palmer/', 'https://www.cnbc.com/2022/12/02/tech-layoffs-leave-visa-holders-scrambling-for-jobs-to-remain-in-us.html', 'https://www.cnbc.com/annie-palmer/', '//www.cnbc.com/us-market-movers/', '/us-market-movers/', '/stocks/', 'https://www.cnbc.com/2022/12/03/the-difference-between-this-comeback-and-the-markets-last-failed-bear-market-bounce.html', '/pro/', 'https://www.cnbc.com/michael-santoli/', 'https://www.cnbc.com/2022/12/02/markets-looking-for-the-next-catalyst-might-latch-on-to-worries-about-2023.html', '/pro/', 'https://www.cnbc.com/patti-domm/', 'https://www.cnbc.com/2022/12/02/the-feds-path-to-a-goldilocks-economy-just-got-more-complicated.html', 'https://www.cnbc.com/jeff-cox/', 'https://www.cnbc.com/2022/12/01/stock-market-futures-open-to-close-news.html', 'https://www.cnbc.com/alex-harring/', 'https://www.cnbc.com/tanaya-macheel/', '//www.cnbc.com/live-tv/', '//www.cnbc.com/latest-video/', 'https://www.cnbc.com/video/2022/12/01/a-rare-interview-with-teslas-chief-designer-franz-von-holzhausen.html', 'https://www.cnbc.com/video/2022/11/28/kellogg-general-mills-and-post-innovating-cereal-amid-declining-sales.html', 'https://www.cnbc.com/video/2022/12/02/cramers-game-plan-for-the-trading-week-of-dec-5.html', 'https://www.cnbc.com/special-reports/', 'https://www.cnbc.com/2022/12/03/i-raised-2-successful-ceos-and-a-doctor-heres-the-no-1-skill-parents-are-failing-to-teach-kids-today.html', 'https://www.cnbc.com/2022/12/04/the-10-countries-with-the-least-paid-vacationthe-us-is-no-2.html', 'https://www.cnbc.com/2022/12/02/harvard-nutritionist-and-brain-expert-shares-worst-foods-that-weaken-memory-and-focus.html', 'https://www.cnbc.com/2022/12/04/manufacturing-orders-from-china-down-40percent-in-demand-collapse.html', 'https://www.cnbc.com/2022/12/04/these-10-cars-have-the-greatest-potential-lifespan.html', 'https://www.cnbc.com/pro/news/', 'https://www.cnbc.com/2022/12/04/death-of-the-internal-combustion-engine-cowens-best-stocks-to-play-the-trend.html', 'https://www.cnbc.com/2022/12/04/death-of-the-internal-combustion-engine-cowens-best-stocks-to-play-the-trend.html', '/pro/', 'https://www.cnbc.com/michelle-fox/', 'https://www.cnbc.com/2022/12/04/these-stocks-are-cheap-heading-into-2023-and-analysts-love-them.html', 'https://www.cnbc.com/2022/12/04/these-stocks-are-cheap-heading-into-2023-and-analysts-love-them.html', '/pro/', 'https://www.cnbc.com/fred-imbert/', 'https://www.cnbc.com/2022/12/03/the-difference-between-this-comeback-and-the-markets-last-failed-bear-market-bounce.html', 'https://www.cnbc.com/2022/12/03/the-difference-between-this-comeback-and-the-markets-last-failed-bear-market-bounce.html', '/pro/', 'https://www.cnbc.com/michael-santoli/', 'https://www.cnbc.com/2022/12/03/goldman-says-buy-these-five-stocks-in-a-weakening-macro-environment.html', 'https://www.cnbc.com/2022/12/03/goldman-says-buy-these-five-stocks-in-a-weakening-macro-environment.html', '/pro/', 'https://www.cnbc.com/michael-bloom/', 'https://www.cnbc.com/2022/12/02/reits-offer-enormous-opportunity-says-gilman-hills-jenny-harrington.html', 'https://www.cnbc.com/2022/12/02/reits-offer-enormous-opportunity-says-gilman-hills-jenny-harrington.html', '/pro/', 'https://www.cnbc.com/michelle-fox/', 'https://www.cnbc.com/sustainable-future/', 'https://www.cnbc.com/2022/11/20/cop27-new-global-climate-deal-struck-at-conference-in-egypt.html', 'https://www.cnbc.com/2022/11/20/cop27-new-global-climate-deal-struck-at-conference-in-egypt.html', 'https://www.cnbc.com/sam-meredith/', 'https://www.cnbc.com/2022/11/18/indiscriminate-use-of-hydrogen-could-slow-energy-transition-report.html', 'https://www.cnbc.com/2022/11/18/indiscriminate-use-of-hydrogen-could-slow-energy-transition-report.html', 'https://www.cnbc.com/anmar-frangoul-profile--cnbc/', 'https://www.cnbc.com/video/2022/11/21/athe-reality-is-earth-is-in-serious-troublea-dr-sylvia-earle-on-our-oceans-in-crisis.html', 'https://www.cnbc.com/video/2022/11/21/athe-reality-is-earth-is-in-serious-troublea-dr-sylvia-earle-on-our-oceans-in-crisis.html', 'https://www.cnbc.com/tania-bryer/', 'https://www.cnbc.com/video/2022/11/21/sylvia-earle-on-the-need-to-protect-our-planet.html', 'https://www.cnbc.com/video/2022/11/21/sylvia-earle-on-the-need-to-protect-our-planet.html', 'https://www.cnbc.com/tania-bryer/', 'https://www.cnbc.com/2022/11/17/cop27-draft-deal-critcized-for-paving-the-way-to-climate-hell.html', 'https://www.cnbc.com/2022/11/17/cop27-draft-deal-critcized-for-paving-the-way-to-climate-hell.html', 'https://www.cnbc.com/sam-meredith/', 'https://www.cnbc.com/coronavirus/', 'https://www.cnbc.com/2022/12/02/biden-administration-will-end-monkeypox-public-health-emergency.html', 'https://www.cnbc.com/2022/12/02/biden-administration-will-end-monkeypox-public-health-emergency.html', 'https://www.cnbc.com/spencer-kimball/', 'https://www.cnbc.com/2022/12/01/covid-fda-pulls-antibody-bebtelovimab-because-not-effective-against-omicron-bqpoint1.html', 'https://www.cnbc.com/2022/12/01/covid-fda-pulls-antibody-bebtelovimab-because-not-effective-against-omicron-bqpoint1.html', 'https://www.cnbc.com/spencer-kimball/', 'https://www.cnbc.com/2022/12/01/china-covid-lockdown-protests-fauci-says-beijing-did-bad-job-vaccinating-elderly.html', 'https://www.cnbc.com/2022/12/01/china-covid-lockdown-protests-fauci-says-beijing-did-bad-job-vaccinating-elderly.html', 'https://www.cnbc.com/spencer-kimball/', 'https://www.cnbc.com/2022/11/30/polio-us-will-test-sewage-for-virus-in-communities-outside-new-york.html', 'https://www.cnbc.com/2022/11/30/polio-us-will-test-sewage-for-virus-in-communities-outside-new-york.html', 'https://www.cnbc.com/spencer-kimball/', 'https://www.cnbc.com/2022/11/30/why-long-covid-could-be-the-next-public-health-disaster.html', 'https://www.cnbc.com/2022/11/30/why-long-covid-could-be-the-next-public-health-disaster.html', 'https://www.cnbc.com/greg-iacurci/', 'https://www.cnbc.com/cnbc-travel/', 'https://www.cnbc.com/2022/11/29/domestic-and-international-tourist-crowds-traveling-in-japan.html', 'https://www.cnbc.com/2022/11/29/domestic-and-international-tourist-crowds-traveling-in-japan.html', 'https://www.cnbc.com/abigail-ng/', 'https://www.cnbc.com/2022/11/22/tickets-on-the-orient-express-are-sold-by-two-companies.html', 'https://www.cnbc.com/2022/11/22/tickets-on-the-orient-express-are-sold-by-two-companies.html', 'https://www.cnbc.com/lucy-handley/', 'https://www.cnbc.com/2022/11/15/private-air-travel-costs-are-increasing-but-flyers-still-want-to-stay.html', 'https://www.cnbc.com/2022/11/15/private-air-travel-costs-are-increasing-but-flyers-still-want-to-stay.html', 'https://www.cnbc.com/monica-pitrelli/', 'https://www.cnbc.com/2022/11/14/is-north-korea-open-to-travelers-no-but-it-may-also-depend-on-china.html', 'https://www.cnbc.com/2022/11/14/is-north-korea-open-to-travelers-no-but-it-may-also-depend-on-china.html', 'https://www.cnbc.com/lee-ying-shan/', 'https://www.cnbc.com/2022/11/10/where-can-disabled-people-easily-travel-survey-names-the-top-cities.html', 'https://www.cnbc.com/2022/11/10/where-can-disabled-people-easily-travel-survey-names-the-top-cities.html', 'https://www.cnbc.com/goh-chiew-tong/', 'https://www.cnbc.com/make-it/', 'https://www.cnbc.com/2022/12/04/the-10-countries-with-the-least-paid-vacationthe-us-is-no-2.html', 'https://www.cnbc.com/2022/12/04/the-10-countries-with-the-least-paid-vacationthe-us-is-no-2.html', 'https://www.cnbc.com/ashton-jackson/', 'https://www.cnbc.com/2022/12/04/michigan-couple-teaches-people-how-to-start-lucrative-side-hustles.html', 'https://www.cnbc.com/2022/12/04/michigan-couple-teaches-people-how-to-start-lucrative-side-hustles.html', 'https://www.cnbc.com/megan-sauer/', 'https://www.cnbc.com/2022/12/04/alef-aeronautics-startup-backed-by-tesla-investor-flying-car-by-2025.html', 'https://www.cnbc.com/2022/12/04/alef-aeronautics-startup-backed-by-tesla-investor-flying-car-by-2025.html', 'https://www.cnbc.com/tom-huddleston-jr/', 'https://www.cnbc.com/2022/12/04/study-exercise-may-increase-the-effectiveness-of-covid-19-vaccines.html', 'https://www.cnbc.com/2022/12/04/study-exercise-may-increase-the-effectiveness-of-covid-19-vaccines.html', 'https://www.cnbc.com/renee-onque/', 'https://www.cnbc.com/2022/12/03/dont-overlook-this-health-and-safety-warning-on-your-holiday-lights.html', 'https://www.cnbc.com/2022/12/03/dont-overlook-this-health-and-safety-warning-on-your-holiday-lights.html', 'https://www.cnbc.com/renee-onque/', 'https://www.cnbc.com/investing-in-supertrends/', 'https://www.cnbc.com/2022/08/18/web3-is-in-chaos-metaverses-in-walled-gardens-randi-zuckerberg.html', 'https://www.cnbc.com/2022/08/18/web3-is-in-chaos-metaverses-in-walled-gardens-randi-zuckerberg.html', 'https://www.cnbc.com/goh-chiew-tong/', 'https://www.cnbc.com/2022/08/17/japan-support-for-nuclear-restart-is-highest-since-fukushima-disaster.html', 'https://www.cnbc.com/2022/08/17/japan-support-for-nuclear-restart-is-highest-since-fukushima-disaster.html', 'https://www.cnbc.com/lee-ying-shan/', 'https://www.cnbc.com/2022/08/10/india-australia-singapore-firms-jobs-are-in-these-sectors-linkedin.html', 'https://www.cnbc.com/2022/08/10/india-australia-singapore-firms-jobs-are-in-these-sectors-linkedin.html', 'https://www.cnbc.com/charmaine-jacob/', 'https://www.cnbc.com/2022/08/08/baidus-robotaxis-dont-need-any-human-staff-in-these-parts-of-china.html', 'https://www.cnbc.com/2022/08/08/baidus-robotaxis-dont-need-any-human-staff-in-these-parts-of-china.html', 'https://www.cnbc.com/evelyn-cheng/', 'https://www.cnbc.com/2021/07/26/action-on-climate-change-can-boost-global-economy-economist-says.html', 'https://www.cnbc.com/2021/07/26/action-on-climate-change-can-boost-global-economy-economist-says.html', '//www.cnbc.com', '//www.cnbc.com/application/pro/?__source=pro|globalfooter/', '//www.cnbc.com/cnbc-reprints/', 'https://www.cnbccouncils.com/', 'https://corporate.comcast.com/values/integrity', 'https://www.peacocktv.com/?cid=20200101evergreensymdisp009&utm_source=cnbc&utm_medium=symphony_editorial_brandawareness_footerlink&utm_campaign=20200101evergreen&utm_term=na&utm_content=na_na/', 'https://cnbcrsh.qualtrics.com/jfe/form/SV_8v2FqPLC71m5Gaq?Origin=cnbc', '//www.cnbc.com/digital-products/', '//www.cnbc.com/cnbc-news-releases/', '//www.cnbc.com/closed-captioning/', '//www.cnbc.com/corrections/', '//www.cnbc.com/about-cnbc-international/', '//www.cnbc.com/cnbc-internship-program/', '//www.cnbc.com/site-map/', 'https://www.nbcuniversal.com/privacy/cookies#cookie_management', '//www.cnbc.com/cnbc-careers-and-employment/', 'https://help.cnbc.com/', 'https://help.cnbc.com/contact/', 'https://www.facebook.com/cnbcinternational/c/', 'https://www.twitter.com/cnbci?lang=en/', 'https://www.linkedin.com/showcase/cnbc-international/', 'https://www.instagram.com/cnbcinternational/?hl=en/', 'https://www.youtube.com/user/CNBCInternational/', 'https://apple.news/T3OtoXcxtRkuHRkM7SpFP_Q', '//www.cnbc.com/rss-feeds/', '//www.cnbc.com/news-tips/', 'https://together.nbcuni.com/advertise/?utm_source=cnbc&utm_medium=referral&utm_campaign=property_ad_pages', '//www.cnbc.com/sign-up-for-cnbc-newsletters/', 'https://www.nbcuniversal.com/privacy?intake=CNBC', 'https://www.nbcuniversal.com/privacy/notrtoo?intake=CNBC', 'https://www.nbcuniversal.com/privacy/california-consumer-privacy-act?intake=CNBC', '/nbcuniversal-terms-of-service/', 'https://www.nbcuniversal.com', '/market-data-terms-of-service/', '//www.cnbc.com/market-data-terms-of-service/']\n"
     ]
    }
   ],
   "source": [
    "#7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "#i) Headline\n",
    "#ii) Time\n",
    "#iii) News Link\n",
    "\n",
    "#7  \n",
    "from bs4 import BeautifulSoup  # import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "page=requests.get(\"https://www.cnbc.com/world/?region=world\")  # send request to get the source code\n",
    "print(page)\n",
    "\n",
    "soup=BeautifulSoup(page.content) # page content\n",
    "soup\n",
    "\n",
    "headline=[]   # make an empty variable\n",
    "for h in soup.find_all('div',class_=\"RiverHeadline-headline RiverHeadline-hasThumbnail\"):  #for loop to scrap the data and put it in variable headline\n",
    "    headline.append(h.get_text())\n",
    "print(\"\\n Headlines: \\n\",headline)  # print data in headline variable\n",
    "\n",
    "time=[]  # make an empty variable\n",
    "for t in soup.find_all('span',class_=\"RiverByline-datePublished\"):  #for loop to scrap the data and put it in variable time\n",
    "    time.append(t.get_text())\n",
    "print(\"\\n Time: \\n\",time) # print data in time variable\n",
    "\n",
    "links = []   # make an empty variable\n",
    "for link in soup.findAll('a'):   #for loop to scrap the data and put it in variable links\n",
    "    links.append(link.get('href'))\n",
    "    \n",
    "print(\"\\n Newslink: \\n\",links) # print data in links variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672deedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "\n",
      " paper_title: \n",
      " ['Reward is enough', 'Making sense of raw input', 'Law and logic: A review from an argumentation perspective', 'Creativity and artificial intelligence', 'Artificial cognition for social human–robot interaction: An implementation', 'Explanation in artificial intelligence: Insights from the social sciences', 'Making sense of sensory input', 'Conflict-based search for optimal multi-agent pathfinding', 'Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning', 'The Hanabi challenge: A new frontier for AI research', 'Evaluating XAI: A comparison of rule-based and example-based explanations', 'Argumentation in artificial intelligence', 'Algorithms for computing strategies in two-player simultaneous move games', 'Multiple object tracking: A literature review', 'Selection of relevant features and examples in machine learning', 'A survey of inverse reinforcement learning: Challenges, methods and progress', 'Explaining individual predictions when features are dependent: More accurate approximations to Shapley values', 'A review of possible effects of cognitive biases on interpretation of rule-based machine learning models', 'Integrating social power into the decision-making of cognitive agents', \"“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems\", 'Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies', 'Algorithm runtime prediction: Methods & evaluation', 'Wrappers for feature subset selection', 'Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics', 'Quantum computation, quantum theory and AI']\n",
      "\n",
      " Authors: \n",
      " ['Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S. ', 'Evans, Richard, Bošnjak, Matko and 5 more', 'Prakken, Henry, Sartor, Giovanni ', 'Boden, Margaret A. ', 'Lemaignan, Séverin, Warnier, Mathieu and 3 more', 'Miller, Tim ', 'Evans, Richard, Hernández-Orallo, José and 3 more', 'Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R. ', 'Sutton, Richard S., Precup, Doina, Singh, Satinder ', 'Bard, Nolan, Foerster, Jakob N. and 13 more', 'van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark ', 'Bench-Capon, T.J.M., Dunne, Paul E. ', 'Bošanský, Branislav, Lisý, Viliam and 3 more', 'Luo, Wenhan, Xing, Junliang and 4 more', 'Blum, Avrim L., Langley, Pat ', 'Arora, Saurabh, Doshi, Prashant ', 'Aas, Kjersti, Jullum, Martin, Løland, Anders ', 'Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Johannes ', 'Pereira, Gonçalo, Prada, Rui, Santos, Pedro A. ', 'Riveiro, Maria, Thill, Serge ', 'Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T. ', 'Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin ', 'Kohavi, Ron, John, George H. ', 'Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna ', 'Ying, Mingsheng ']\n",
      "\n",
      " Published_date: \n",
      " ['October 2021', 'October 2021', 'October 2015', 'August 1998', 'June 2017', 'February 2019', 'April 2021', 'February 2015', 'August 1999', 'March 2020', 'February 2021', 'October 2007', 'August 2016', 'April 2021', 'December 1997', 'August 2021', 'September 2021', 'June 2021', 'December 2016', 'September 2021', 'May 2021', 'January 2014', 'December 1997', 'October 2021', 'February 2010']\n",
      "\n",
      " Paper_url: \n",
      " ['#skip-to-content-anchor', 'http://www.elsevier.com', 'https://account.elsevier.com/auth', 'https://elsevier.com/about', 'https://www.elsevier.com/connect', 'https://www.elsevier.com/about/careers', 'https://elsevier.com/about', 'https://www.elsevier.com/connect', 'https://www.elsevier.com/about/careers', 'https://www.elsevier.com/rd-solutions', 'https://www.elsevier.com/clinical-solutions', 'https://www.elsevier.com/research-platforms', 'https://www.elsevier.com/research-intelligence', 'https://www.elsevier.com/education', 'https://www.elsevier.com/solutions', 'https://www.elsevier.com/rd-solutions', 'https://www.elsevier.com/clinical-solutions', 'https://www.elsevier.com/research-platforms', 'https://www.elsevier.com/research-intelligence', 'https://www.elsevier.com/education', 'https://www.elsevier.com/solutions', 'https://www.elsevier.com/authors', 'https://www.elsevier.com/editors', 'https://www.elsevier.com/reviewers', 'https://www.elsevier.com/librarians', 'https://www.elsevier.com/strategic-partners', 'https://www.elsevier.com/open-access', 'https://www.elsevier.com/societies', 'https://www.elsevier.com/authors', 'https://www.elsevier.com/editors', 'https://www.elsevier.com/reviewers', 'https://www.elsevier.com/librarians', 'https://www.elsevier.com/strategic-partners', 'https://www.elsevier.com/open-access', 'https://www.elsevier.com/societies', 'https://www.elsevier.com/books-and-journals', 'https://webshop.elsevier.com/?utm_source=ecom&utm_medium=top&utm_campaign=webshop', 'https://www.elsevier.com/books-and-journals', 'https://webshop.elsevier.com/?utm_source=ecom&utm_medium=top&utm_campaign=webshop', 'https://global-checkout.elsevier.com', 'https://account.elsevier.com/auth', 'https://www.sciencedirect.com/science/journal/00043702', 'https://www.editorialmanager.com/artint/default.aspx', 'https://www.elsevier.com/', 'https://www.elsevier.com/search-results?labels=journals', '/artificial-intelligence', '/artificial-intelligence/most-downloaded-articles', 'https://www.sciencedirect.com/science/journal/00043702', 'https://www.editorialmanager.com/artint/default.aspx', 'https://www.sciencedirect.com/science/journal/00043702', 'https://www.elsevier.com/journals/artificial-intelligence/0004-3702/guide-for-authors', 'https://www.editorialmanager.com/artint/default.aspx', 'https://authors.elsevier.com/tracking/landingpage/selection.do', 'https://www.elsevier.com/journals/artificial-intelligence/0004-3702/subscribe?subscriptiontype=institutional', 'https://www.sciencedirect.com/science/article/pii/S0004370221000862', 'https://www.sciencedirect.com/science/article/pii/S0004370221000722', 'https://www.sciencedirect.com/science/article/pii/S0004370215000910', 'https://www.sciencedirect.com/science/article/pii/S0004370298000551', 'https://www.sciencedirect.com/science/article/pii/S0004370216300790', 'https://www.sciencedirect.com/science/article/pii/S0004370218305988', 'https://www.sciencedirect.com/science/article/pii/S0004370220301855', 'https://www.sciencedirect.com/science/article/pii/S0004370214001386', 'https://www.sciencedirect.com/science/article/pii/S0004370299000521', 'https://www.sciencedirect.com/science/article/pii/S0004370219300116', 'https://www.sciencedirect.com/science/article/pii/S0004370220301533', 'https://www.sciencedirect.com/science/article/pii/S0004370207000793', 'https://www.sciencedirect.com/science/article/pii/S0004370216300285', 'https://www.sciencedirect.com/science/article/pii/S0004370220301958', 'https://www.sciencedirect.com/science/article/pii/S0004370297000635', 'https://www.sciencedirect.com/science/article/pii/S0004370221000515', 'https://www.sciencedirect.com/science/article/pii/S0004370221000539', 'https://www.sciencedirect.com/science/article/pii/S0004370221000096', 'https://www.sciencedirect.com/science/article/pii/S0004370216300868', 'https://www.sciencedirect.com/science/article/pii/S0004370221000588', 'https://www.sciencedirect.com/science/article/pii/S0004370221000102', 'https://www.sciencedirect.com/science/article/pii/S0004370213001082', 'https://www.sciencedirect.com/science/article/pii/S000437029700043X', 'https://www.sciencedirect.com/science/article/pii/S0004370221000734', 'https://www.sciencedirect.com/science/article/pii/S0004370209001398', 'https://www.sciencedirect.com/science/journal/00043702', 'https://www.sciencedirect.com/user/alerts', 'https://www.sciencedirect.com/user/register?utm_campaign=sd_recommender_ELSJLS&utm_channel=elseco&dgcid=sd_recommender_ELSJLS', 'http://www.elsevier.com/authors/home', 'https://www.editorialmanager.com/artint/default.aspx', 'https://www.editorialmanager.com/artint/default.aspx', 'https://researcheracademy.elsevier.com', 'https://www.elsevier.com/about/policies/copyright/permissions', 'https://webshop.elsevier.com', 'https://service.elsevier.com/app/home/supporthub/publishing/#authors', 'https://authors.elsevier.com/tracking/landingpage/selection.do', 'https://www.elsevier.com/librarians', 'https://www.elsevier.com/journals/artificial-intelligence/0004-3702/subscribe?subscriptiontype=institutional', 'http://www.elsevier.com/editors', 'http://www.elsevier.com/editors/perk', 'https://www.elsevier.com/editors/guest-editors', 'https://service.elsevier.com/app/home/supporthub/publishing/#editors', 'http://www.elsevier.com/reviewers', 'https://www.elsevier.com/reviewers/how-to-review', 'https://www.editorialmanager.com/artint/default.aspx', 'https://www.elsevier.com/reviewers/becoming-a-reviewer-how-and-why#recognizing', 'https://service.elsevier.com/app/home/supporthub/publishing/#reviewers', 'https://www.elsevier.com', '//www.elsevier.com/legal/elsevier-website-terms-and-conditions', '//www.elsevier.com/legal/privacy-policy', '//www.elsevier.com/legal/cookienotice', '//www.elsevier.com/sitemap', 'https://www.relx.com/']\n"
     ]
    }
   ],
   "source": [
    "#8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "#https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "#Scrape below mentioned details :\n",
    "#i) Paper Title ii) Authors iii) Published Date iv) Paper URL\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup   # import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "page=requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")  # send requests to get the source code\n",
    "print(page)\n",
    "\n",
    "soup=BeautifulSoup(page.content)  # page content\n",
    "soup\n",
    "\n",
    "paper_title=[]   # make an empty variable\n",
    "for p in soup.find_all('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):  #for loop to scrap the data and put it in variable paper_title\n",
    "    paper_title.append(p.get_text())\n",
    "print(\"\\n paper_title: \\n\",paper_title)  # print data in paper_title variable\n",
    "\n",
    "authors=[]   # make an empty variable\n",
    "for a in soup.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):   #for loop to scrap the data and put it in variable authors\n",
    "    authors.append(a.get_text())\n",
    "print(\"\\n Authors: \\n\",authors)    # print data in authors variable\n",
    "\n",
    "publisheddate=[]     # make an empty variable\n",
    "for d in soup.find_all('span',class_=\"sc-1thf9ly-2 dvggWt\"):    #for loop to scrap the data and put it in variable publisheddate\n",
    "    publisheddate.append(d.get_text())\n",
    "print(\"\\n Published_date: \\n\",publisheddate)  # print data in publishedate variable\n",
    "\n",
    "paperurl=[]     # make an empty variable\n",
    "for u in soup.findAll('a'):   #for loop to scrap the data and put it in variable paperurl\n",
    "    paperurl.append(u.get('href'))\n",
    "print(\"\\n Paper_url: \\n\",paperurl)    # print data in paperurl variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cd5a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Restraunt_name: \n",
      " ['Castle Barbeque', 'Jungle Jamboree', 'Castle Barbeque', 'Cafe Knosh', 'The Barbeque Company', 'India Grill', 'Delhi Barbeque', 'The Monarch - Bar Be Que Village', 'Indian Grill Room']\n",
      "\n",
      " Location: \n",
      " ['Connaught Place, Central Delhi', '3CS Mall,Lajpat Nagar - 3, South Delhi', 'Pacific Mall,Tagore Garden, West Delhi', 'The Leela Ambience Convention Hotel,Shahdara, East Delhi', 'Gardens Galleria,Sector 38A, Noida', 'Hilton Garden Inn,Saket, South Delhi', 'Taurus Sarovar Portico,Mahipalpur, South Delhi', 'Indirapuram Habitat Centre,Indirapuram, Ghaziabad', 'Suncity Business Tower,Golf Course Road, Gurgaon']\n",
      "\n",
      " ratings: \n",
      " ['4.1', '3.9', '3.9', '4.3', '4', '3.9', '3.6', '3.8', '4.3']\n",
      "\n",
      " Images_url: \n",
      " ['https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/k/b/p86792-16062953735fbe1f4d3fb7e.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/p/m/p59633-166088382462ff137009010.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/j/o/p38113-15959192065f1fcb666130c.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/p/m/p406-15438184745c04ccea491bc.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/p/k/p79307-16051787755fad1597f2bf9.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/v/t/p2687-1482477169585cce712b90f.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/d/i/p52501-1661855212630de5eceb6d2.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/o/p34822-15599107305cfa594a13c24.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/y/f/p549-165000147262590640c0afc.jpg?tr=tr:n-medium']\n",
      "\n",
      "Cuisine:\n",
      " [' Chinese, North Indian', ' North Indian, Asian, Italian', ' Chinese, North Indian', ' Italian, Continental', ' North Indian, Chinese', ' North Indian, Italian', ' North Indian', ' North Indian', ' North Indian, Mughlai']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rstraunt name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Images_url</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Rstraunt name  \\\n",
       "0                   Castle Barbeque   \n",
       "1                   Jungle Jamboree   \n",
       "2                   Castle Barbeque   \n",
       "3                        Cafe Knosh   \n",
       "4              The Barbeque Company   \n",
       "5                       India Grill   \n",
       "6                    Delhi Barbeque   \n",
       "7  The Monarch - Bar Be Que Village   \n",
       "8                 Indian Grill Room   \n",
       "\n",
       "                                            Location  \\\n",
       "0                     Connaught Place, Central Delhi   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi   \n",
       "2             Pacific Mall,Tagore Garden, West Delhi   \n",
       "3  The Leela Ambience Convention Hotel,Shahdara, ...   \n",
       "4                 Gardens Galleria,Sector 38A, Noida   \n",
       "5               Hilton Garden Inn,Saket, South Delhi   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon   \n",
       "\n",
       "                         Cuisine  \\\n",
       "0          Chinese, North Indian   \n",
       "1   North Indian, Asian, Italian   \n",
       "2          Chinese, North Indian   \n",
       "3           Italian, Continental   \n",
       "4          North Indian, Chinese   \n",
       "5          North Indian, Italian   \n",
       "6                   North Indian   \n",
       "7                   North Indian   \n",
       "8          North Indian, Mughlai   \n",
       "\n",
       "                                          Images_url Ratings  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...     4.1  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...     3.9  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...     3.9  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...     4.3  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...       4  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...     3.9  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...     3.6  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...     3.8  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...     4.3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "#i) Restaurant name ii) Cuisine iii) Location iv) Ratings v) Image URL\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup   # import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "page=requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\") # send request to get the source code\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)   # page content\n",
    "soup\n",
    "\n",
    "\n",
    "name=[]  # make an empty variable\n",
    "for n in soup.find_all('a',class_=\"restnt-name ellipsis\"): #for loop to scrap the data and put it in variable name\n",
    "    name.append(n.get_text().strip())\n",
    "print(\"\\n Restraunt_name: \\n\",name)  # print name\n",
    "\n",
    "\n",
    "location=[]   # make an empty variable\n",
    "\n",
    "for l in soup.find_all('div',class_=\"restnt-loc ellipsis\"):  #for loop to scrap the data and put it in variable location\n",
    "    location.append(l.get_text())\n",
    "print(\"\\n Location: \\n\",location)    # print location\n",
    "\n",
    "\n",
    "ratings=[]    # make an empty variable\n",
    "\n",
    "for r in soup.find_all('div',class_=\"restnt-rating rating-4\"):   #for loop to scrap the data and put it in variable rating\n",
    "    ratings.append(r.get_text())\n",
    "print(\"\\n ratings: \\n\",ratings)  # print rating\n",
    "\n",
    "\n",
    "images=[]    # make an empty variable\n",
    "\n",
    "for i in soup.find_all('img',class_=\"no-img\"): #for loop to scrap the data and put it in variable images\n",
    "    images.append(i.get('data-src'))\n",
    "print(\"\\n Images_url: \\n\",images)    # print images\n",
    "\n",
    "cuisine=[]    # make an empty variable\n",
    "\n",
    "for c in soup.find_all('span',class_=\"double-line-ellipsis\"):   #for loop to scrap the data and put it in variable cuisine\n",
    "    cuisine.append(c.text.split('|')[1])\n",
    "print(\"\\nCuisine:\\n\",cuisine)  # print cuisine\n",
    "\n",
    "df= pd.DataFrame({'Rstraunt name':name,'Location':location,'Cuisine':cuisine,'Images_url':images,'Ratings':ratings})  # make dataframe\n",
    "df   # print dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92757671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Rank: \n",
      " ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.', '16.', '17.', '18.', '19.', '20.', '21.', '22.', '23.', '24.', '25.', '26.', '27.', '28.', '29.', '30.', '31.', '32.', '33.', '34.', '35.', '36.', '37.', '38.', '39.', '40.', '41.', '42.', '43.', '44.', '45.', '46.', '47.', '48.', '49.', '50.', '51.', '52.', '53.', '54.', '55.', '56.', '57.', '58.', '59.', '60.', '61.', '62.', '63.', '64.', '65.', '66.', '67.', '68.', '69.', '70.', '71.', '72.', '73.', '74.', '75.', '76.', '77.', '78.', '79.', '80.', '81.', '82.', '83.', '84.', '85.', '86.', '87.', '88.', '89.', '90.', '91.', '92.', '93.', '94.', '95.', '96.', '97.', '98.', '99.', '100.']\n",
      "\n",
      " Publication: \n",
      " ['Nature', 'The New England Journal of Medicine', 'Science', 'IEEE/CVF Conference on Computer Vision and Pattern Recognition', 'The Lancet', 'Advanced Materials', 'Nature Communications', 'Cell', 'International Conference on Learning Representations', 'Neural Information Processing Systems', 'JAMA', 'Chemical Reviews', 'Proceedings of the National Academy of Sciences', 'Angewandte Chemie', 'Chemical Society Reviews', 'Journal of the American Chemical Society', 'IEEE/CVF International Conference on Computer Vision', 'Nucleic Acids Research', 'International Conference on Machine Learning', 'Nature Medicine', 'Renewable and Sustainable Energy Reviews', 'Science of The Total Environment', 'Advanced Energy Materials', 'Journal of Clinical Oncology', 'ACS Nano', 'Journal of Cleaner Production', 'Advanced Functional Materials', 'Physical Review Letters', 'Scientific Reports', 'The Lancet Oncology', 'Energy & Environmental Science', 'IEEE Access', 'PLoS ONE', 'Science Advances', 'Journal of the American College of Cardiology', 'Applied Catalysis B: Environmental', 'Nature Genetics', 'BMJ', 'Circulation', 'European Conference on Computer Vision', 'International Journal of Molecular Sciences', 'Nature Materials', 'Chemical engineering journal', 'AAAI Conference on Artificial Intelligence', 'Journal of Materials Chemistry A', 'ACS Applied Materials & Interfaces', 'Nature Biotechnology', 'The Lancet Infectious Diseases', 'Frontiers in Immunology', 'Applied Energy', 'Nano Energy', 'Nature Energy', 'Meeting of the Association for Computational Linguistics (ACL)', 'The Astrophysical Journal', 'Gastroenterology', 'Nature Methods', 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'Cochrane Database of Systematic Reviews', 'Blood', 'Neuron', 'Nano Letters', 'Morbidity and Mortality Weekly Report', 'European Heart Journal', 'Nature Nanotechnology', 'ACS Catalysis', 'Nature Neuroscience', 'American Economic Review', 'Journal of High Energy Physics', 'IEEE Communications Surveys & Tutorials', 'Annals of Oncology', 'Nutrients', 'Accounts of Chemical Research', 'Immunity', 'Environmental Science & Technology', 'Nature Reviews. Molecular Cell Biology', 'Gut', 'Physical Review D', 'ACS Energy Letters', 'Monthly Notices of the Royal Astronomical Society', 'Conference on Empirical Methods in Natural Language Processing (EMNLP)', 'Clinical Infectious Diseases', 'Cell Metabolism', 'Nature Reviews Immunology', 'Joule', 'Nature Photonics', 'International Journal of Environmental Research and Public Health', 'Environmental Pollution', 'Computers in Human Behavior', 'Frontiers in Microbiology', 'Nature Physics', 'Small', 'Cell Reports', 'Molecular Cell', 'Clinical Cancer Research', 'Bioresource Technology', 'Journal of Business Research', 'Molecular Cancer', 'Sensors', 'Nature Climate Change', 'IEEE Internet of Things Journal']\n",
      "\n",
      " h5_index: \n",
      " ['444', '432', '401', '389', '354', '312', '307', '300', '286', '278', '267', '265', '256', '245', '244', '242', '239', '238', '237', '235', '227', '225', '220', '213', '211', '211', '210', '207', '206', '202', '202', '200', '198', '197', '195', '192', '191', '190', '189', '186', '183', '181', '181', '180', '178', '177', '175', '173', '173', '173', '172', '170', '169', '167', '166', '165', '165', '165', '165', '164', '164', '163', '163', '163', '163', '162', '160', '160', '159', '159', '159', '159', '158', '158', '155', '155', '155', '155', '155', '154', '153', '153', '152', '152', '152', '152', '152', '152', '151', '151', '150', '149', '149', '146', '146', '145', '145', '145', '144', '144']\n",
      "\n",
      " h5_median: \n",
      " ['667', '780', '614', '627', '635', '418', '428', '505', '533', '436', '425', '444', '364', '332', '386', '344', '415', '550', '421', '389', '324', '311', '300', '315', '277', '273', '280', '294', '274', '329', '290', '303', '278', '294', '276', '246', '297', '307', '301', '321', '253', '265', '224', '296', '220', '223', '315', '296', '228', '217', '232', '314', '304', '234', '254', '296', '293', '243', '229', '231', '207', '302', '265', '264', '220', '248', '263', '220', '304', '243', '214', '211', '242', '214', '340', '235', '217', '212', '194', '249', '278', '211', '292', '233', '228', '225', '222', '214', '225', '222', '196', '205', '202', '201', '190', '233', '209', '201', '228', '212']\n"
     ]
    }
   ],
   "source": [
    "#10) Write a python program to scrape the details of top publications from Google Scholar from https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "#i) Rank ii) Publication iii) h5-index iv) h5-median\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup   # import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "page=requests.get(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")# send request to get the source code\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content)  # page content\n",
    "soup\n",
    "\n",
    "rank=[]   # make an empty variable rank\n",
    "for r in soup.find_all('td',class_=\"gsc_mvt_p\"):  # for loop to get the scrapped data and put in an variable\n",
    "    rank.append(r.get_text())\n",
    "    \n",
    "print(\"\\n Rank: \\n\",rank)   # print rank\n",
    "\n",
    "\n",
    "publication=[]   # make an empty variable publication\n",
    "\n",
    "for p in soup.find_all('td',class_=\"gsc_mvt_t\"): # for loop to get the scrapped data and put in an variable\n",
    "    publication.append(p.get_text().strip())\n",
    "    \n",
    "print(\"\\n Publication: \\n\",publication)  # print publication\n",
    "\n",
    "\n",
    "h5_index=[]    # make an empty variable h5_index\n",
    "\n",
    "for i in soup.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):  # for loop to get the scrapped data and put in an variable\n",
    "    h5_index.append(i.get_text())\n",
    "    \n",
    "print(\"\\n h5_index: \\n\",h5_index) # print h5_index\n",
    "\n",
    "\n",
    "h5_median=[]   # make an empty variable h5_median\n",
    "\n",
    "for i in soup.find_all('span',class_=\"gs_ibl gsc_mp_anchor\"):# for loop to get the scrapped data and put in an variable\n",
    "    h5_median.append(i.get_text())\n",
    "    \n",
    "print(\"\\n h5_median: \\n\",h5_median)# print h5_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a54727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
